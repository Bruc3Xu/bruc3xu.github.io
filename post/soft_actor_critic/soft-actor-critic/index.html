<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>Soft Actor Critic - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content="现有深度强化学习算法主要的问题有：
 采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。 "><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/soft_actor_critic/soft-actor-critic/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="Soft Actor Critic">
<meta property="og:description" content="现有深度强化学习算法主要的问题有：

采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。
">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/soft_actor_critic/soft-actor-critic/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-11-13T09:34:56+08:00">
<meta property="article:modified_time" content="2020-11-13T09:34:56+08:00">
<meta itemprop=name content="Soft Actor Critic">
<meta itemprop=description content="现有深度强化学习算法主要的问题有：

采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。
"><meta itemprop=datePublished content="2020-11-13T09:34:56+08:00">
<meta itemprop=dateModified content="2020-11-13T09:34:56+08:00">
<meta itemprop=wordCount content="887">
<meta itemprop=keywords content="reinforcement learning,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Soft Actor Critic">
<meta name=twitter:description content="现有深度强化学习算法主要的问题有：

采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。
"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>Soft Actor Critic</h1>
<div class=post-meta>
<span class=post-time> 2020-11-13 </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#idea>Idea</a>
<ul>
<li><a href=#maximum-entropy-framework>Maximum Entropy Framework</a></li>
<li><a href=#algorithm>Algorithm</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>现有深度强化学习算法主要的问题有：</p>
<ul>
<li>采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。</li>
</ul>
<ul>
<li>训练不稳定，收敛困难：off-policy目的在于利用不同于当下策略采集的样本来学习。</li>
</ul>
<h2 id=idea>Idea</h2>
<p>Soft Actor Critic是基于最大熵框架的off-policy、actor-critic算法。</p>
<h3 id=maximum-entropy-framework>Maximum Entropy Framework</h3>
<p><em>&ldquo;Succeed at the task, while behaving as random as possible&rdquo;</em></p>
<p>Actor在最大化期望奖励的同时最大化熵：</p>
<ul>
<li>更多的exploration避免遗漏更好的trajectory。</li>
<li>学习得到的策略更加鲁棒，因为学习过程中尽可能采取随机的策略（可以看做噪声更多），那么得到的策略在实际测试中会更加具有泛化性。</li>
</ul>
<p>优化目标变为：</p>
<p>$$
J(\pi) = \sum_t E_{(s_t, a_t) \sim \rho_{\pi}} \left[ r(s_t, a_t) + \alpha H(\pi(\cdot | s_t)) \right]
$$</p>
<p>其中$\alpha$是"temperature"参数，可以固定或者自动学习（对应不同版本的SAC算法）。</p>
<p>Bellman <a href=https://en.wikipedia.org/wiki/Operator_(mathematics)>operator</a> $\mathcal{T}$：</p>
<p>$$
\mathcal{T}^\pi Q(s_t, a_t) = r(s_t, a_t) + \gamma E_{s+1 \sim p} \left[ V (s_{t+1}) \right]
$$</p>
<p>其中：</p>
<p>$$
V (s_{t+1}) = E_{a_t \sim \pi} \left[ Q (s_t, a_t) \right] - \alpha E_{a_t \sim \pi} \left[ \log \pi (a_t | s_t) \right]
$$</p>
<p>下一个状态的价值等于所有动作价值加上当前状态熵的期望。</p>
<p>分布$X$的<a href=https://en.wikipedia.org/wiki/Entropy_(information_theory)>entropy</a>定义为$H(X) = - E_x (\log(X))$。</p>
<h3 id=algorithm>Algorithm</h3>
<p>SAC算法是Actor-Critic结构，因而包含：</p>
<ul>
<li>Soft值函数：$V_\psi$</li>
<li>SoftQ函数： $Q_\theta$</li>
<li>策略： $\pi_\phi$</li>
</ul>
<p>尽管同时学习$V_\psi$和$Q_\theta$显得多余，但可以稳定训练过程。</p>
<p>$V_\psi$的优化目标为最小化<a href=https://en.wikipedia.org/wiki/Residual_sum_of_squares>RSS</a>：</p>
<p>$$
J_V (\psi) = E_{s_t \sim \mathcal{D}} \left[ \frac{1}{2} \left(V_\psi (s_t) - E_{a_t \sim \pi_\phi} \left[ Q_\theta (s_t, a_t) - \log \pi_\phi (a_t | s_t) \right] \right)^2 \right]
$$</p>
<p>$Q_\theta$的优化目标为最小化<a href=https://en.wikipedia.org/wiki/Residual_sum_of_squares>RSS</a>：</p>
<p>$$
J_Q (\theta) = E_{s_t \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_\theta (s_t, a_t) - r(s_t, a_t) - \gamma E_{s+1 \sim p} \left[ V (s_{t+1}) \right] \right)^2 \right]
$$</p>
<p>$\psi$参数更新使用了Soft更新（Polyak），$\bar{\psi}$代表更新后的网络参数。</p>
<p>$$
J_\pi (\phi) = E_{s_t \sim \mathcal{D}} \left[ D_{KL} \left(\pi_\phi (\cdot|s_t) \mid \mid \frac{exp( Q_{\theta} (s_t, \cdot)}{Z_\theta} \right) \right]
$$</p>
<p>$\frac{exp( Q_{\theta} (s_t, \cdot)}{Z_\theta}$是基于玻尔兹曼分布的探索策略，能够描述多模态策略，体现了Soft这一特点。
详见<a href=https://zhuanlan.zhihu.com/p/70360272>SAC</a>。</p>
<p><img src=/post/soft_actor_critic/projection.png alt></p>
<p>SAC算法：</p>
<p><img src=/post/soft_actor_critic/algorithm.png alt></p>
<p>两个Q函数用来加速训练，损失计算中target是取二者Q中较小值。</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2020-11-13
</span>
</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/%E4%BD%BF%E7%94%A8gitflow%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%8F%91/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">使用gitflow模式开发</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/distributed_rl/distributed_rl/>
<span class="next-text nav-default">Distributed RL</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>