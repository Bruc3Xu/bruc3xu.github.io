<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>Model-based RL - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content="{% include start-row.html %}
Idea: If we learn $$f(s_t, a_t) = s_{t+1}$$ (or $$p(s_{t+1} \mid s_t, a_t)$$ in stochastic envs.) we can apply last lecture techniques to get a policy.
Basic approaches System identification in classical robotics: {% include figure.html url=&amp;quot;/_rl/lecture_11/si_1.png&amp;quot;%}
Does it work? Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters."><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_chapter9/lecture11/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="Model-based RL">
<meta property="og:description" content="{% include start-row.html %}
Idea: If we learn $$f(s_t, a_t) = s_{t+1}$$ (or $$p(s_{t+1} \mid s_t, a_t)$$ in stochastic envs.) we can apply last lecture techniques to get a policy.
Basic approaches System identification in classical robotics: {% include figure.html url=&#34;/_rl/lecture_11/si_1.png&#34;%}
Does it work? Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_chapter9/lecture11/"><meta property="article:section" content="post">
<meta itemprop=name content="Model-based RL">
<meta itemprop=description content="{% include start-row.html %}
Idea: If we learn $$f(s_t, a_t) = s_{t+1}$$ (or $$p(s_{t+1} \mid s_t, a_t)$$ in stochastic envs.) we can apply last lecture techniques to get a policy.
Basic approaches System identification in classical robotics: {% include figure.html url=&#34;/_rl/lecture_11/si_1.png&#34;%}
Does it work? Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters.">
<meta itemprop=wordCount content="1494">
<meta itemprop=keywords content><meta name=twitter:card content="summary">
<meta name=twitter:title content="Model-based RL">
<meta name=twitter:description content="{% include start-row.html %}
Idea: If we learn $$f(s_t, a_t) = s_{t+1}$$ (or $$p(s_{t+1} \mid s_t, a_t)$$ in stochastic envs.) we can apply last lecture techniques to get a policy.
Basic approaches System identification in classical robotics: {% include figure.html url=&#34;/_rl/lecture_11/si_1.png&#34;%}
Does it work? Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>Model-based RL</h1>
<div class=post-meta>
<span class=post-time> 0001-01-01 </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li><a href=#basic-approaches>Basic approaches</a>
<ul>
<li>
<ul>
<li><a href=#system-identification-in-classical-robotics>System identification in classical robotics:</a></li>
<li><a href=#model-predictive-controller-mpc>Model Predictive Controller (MPC)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#uncertainty-in-model-based-rl>Uncertainty in model-based RL</a>
<ul>
<li><a href=#estimating-the-posterior-ptheta-mid-mathcald>Estimating the posterior: $$p(\theta \mid \mathcal{D})$$</a>
<ul>
<li><a href=#bootstrap-ensembling>Bootstrap ensembling</a></li>
<li><a href=#planning-with-uncertainty>Planning with uncertainty</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#model-based-rl-with-complex-observations>Model-based RL with complex observations</a>
<ul>
<li>
<ul>
<li><a href=#latent-space-models>Latent space models</a></li>
<li><a href=#observations-model>Observations model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>{% include start-row.html %}</p>
<p><strong>Idea:</strong> If we learn $$f(s_t, a_t) = s_{t+1}$$ (or $$p(s_{t+1} \mid s_t, a_t)$$ in stochastic envs.) we can apply <a href=/lectures/lecture10>last lecture</a> techniques to get a policy.</p>
<h1 id=basic-approaches>Basic approaches</h1>
<h3 id=system-identification-in-classical-robotics>System identification in classical robotics:</h3>
<p>{% include figure.html url="/_rl/lecture_11/si_1.png"%}</p>
<p><strong>Does it work?</strong>
Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters.</p>
<p><strong>Distribution mismatch problem:</strong> When we design a policy, we might be extrapolating beyond the data distribution used to learn the physics model, making results to be far off.</p>
<p><strong>Solution:</strong> (Inspired by DAgger) Add samples from estimated optimal trajectory.</p>
<p>{% include figure.html url="/_rl/lecture_11/si_2.png"%}</p>
<p><strong>Problem:</strong> Blindly planning on top of an imperfect learned env. model might still result into ending up taking actions in states we did not plan to be. To improve this, we can update the plan after each step:</p>
<h3 id=model-predictive-controller-mpc>Model Predictive Controller (MPC)</h3>
<p>{% include end-row.html %}
{% include start-row.html %}</p>
<p>{% include figure.html url="/_rl/lecture_11/si_3.png"%}</p>
<p>{% include annotation.html %}
This algorithm works very decently: Replanning after each step compensates for imperfections in model and in planner. We can even plan for shorter horizons or use some very simple planner like random shooting.
{% include end-row.html %}
{% include start-row.html %}</p>
<p><strong>Problems:</strong></p>
<ul>
<li>Replanning after every step can be very expensive if we use good planners.</li>
<li>Model uncertainty may lead into overestimating regions resulting into the algorithm not progressing.</li>
</ul>
<h1 id=uncertainty-in-model-based-rl>Uncertainty in model-based RL</h1>
<p><strong>Idea:</strong> What if we exploit models being aware of their uncertainty? (similarly to Gaussian Processes)</p>
<p>{% include end-row.html %}
{% include start-row.html %}</p>
<p>There exist 2 types of uncertainty:</p>
<ul>
<li>The <strong>aleatoric</strong> (or statistical) uncertainty: Regarding the samples of the data. A good model of high aleatoric uncertainty data can have relative high entropy.</li>
<li>The <strong>epistemic</strong> (or model) uncertainty: When the model is certain about the data, but we are not certain about the model (e.g. overfiting). You cannot infer that uncertainty from output entropy.</li>
</ul>
<p>{% include annotation.html %}
We cannot just use entropy of the ANN outcome since it is not a good measure of uncertainty. The model could be overfited and be very confident about something it is wrong about.
{% include end-row.html %}
{% include start-row.html %}</p>
<h2 id=estimating-the-posterior-ptheta-mid-mathcald>Estimating the posterior: $$p(\theta \mid \mathcal{D})$$</h2>
<p>When training models, we usually only get a Maximum Likelihood Estimation (MLE) of the params: $$\arg \max_\theta \log p(\theta \mid \mathcal{D})$$ (which if the priors are uniform is the same as $$\arg \max_\theta \log p(\mathcal{D} \mid \theta)$$).\<br>
Nevertheless, $$p(\theta \mid \mathcal{D})$$ would tell us the real uncertainty of the model. Moreover, we could make predictions marginalizing over the parameters:</p>
<p>\begin{equation}
p(s_{t+1} \mid s_t, a_t) = \int p(s_{t+1} \mid s_t, a_t, \theta) p(\theta \mid \mathcal{D}) d\theta
\end{equation}</p>
<h3 id=bootstrap-ensembling>Bootstrap ensembling</h3>
<p>We can achieve a rough approximation of $$p(s_{t+1} \mid s_t, a_t)$$ by training multiple independent models and averaging them as:</p>
<p>\begin{equation}
\label{bootstrap}
\int p(s_{t+1} \mid s_t, a_t, \theta) p(\theta \mid \mathcal{D}) d\theta \simeq
\frac{1}{N} \sum_i p(s_{t+1} \mid s_t, a_t, \theta_i)
\end{equation}</p>
<p>{% include end-row.html %}
{% include start-row.html %}</p>
<p>Bootstrap ensembling achieves the training of &ldquo;independent&rdquo; models by training on &ldquo;independent&rdquo; datasets:
It uses <strong>bootstrap samples</strong>, uniform sampling with replacement of the entire dataset.
Later, to get an estimation of $$p(\theta \mid \mathcal{D})$$ we can just average each model output as depicted in Eq. \ref{bootstrap}.</p>
<p>{% include annotation.html %}
Bootstrap sampling is not very needed since SGD and random initialization is usually sufficient to make models independent enough.
{% include end-row.html %}
{% include start-row.html %}</p>
<p><strong>Problem:</strong> Training ANNs is expensive so usually the number of fitted models is very small (&lt; 10), this makes the approximation to be very crude.</p>
<p><strong>Solution:</strong> In later lectures we will present better methods such as the use of <strong>Bayesian NN (BNN)</strong>, <strong>Mean Field approximations</strong> and <strong>variational inference</strong>.</p>
<h3 id=planning-with-uncertainty>Planning with uncertainty</h3>
<h4 id=deterministic-environments>Deterministic environments</h4>
<p><a href=/lectures/lecture10>Before</a> a single model predicted next state:</p>
<p>\begin{equation}
J(a_1,&mldr;,a_T) = \sum_t r(s_t, a_t)
\space \space \space \space s.t. \space
s_{t+1} = f(s_t, a_t)
\end{equation}</p>
<h4 id=stochastic-environments>Stochastic environments:</h4>
<p>Now, the next states are predicted by an average of multiple models:
\begin{equation}
J(a_1,&mldr;,a_T) = \frac{1}{N} \sum_i \sum_t r(s_{t, i}, a_t)
\space \space \space \space s.t. \space
s_{t+1, i} = f_i (s_{t, i}, a_t)
\end{equation}</p>
<p>In case of a stochastic environment, the algorithm becomes:
{% include end-row.html %}
{% include start-row.html %}</p>
<p>{% include figure.html url="/_rl/lecture_11/stoc_bootstrap.png"%}</p>
<p>{% include annotation.html %}
There exist fancier options such as <strong>moment matching</strong> or better posterior estimation with <strong>BNNs</strong>.
{% include end-row.html %}
{% include start-row.html %}</p>
<h1 id=model-based-rl-with-complex-observations>Model-based RL with complex observations</h1>
<p><strong>Problems</strong> in estimating $$f(s_t, a_t) = s_{t+1}$$ big observation spaces such as images:</p>
<ul>
<li><strong>High dimensionality</strong> makes it harder to fit an environment model.</li>
<li><strong>Redundancies</strong>: Some pixel values behave the same way even if far apart because of intrinsic correlations.</li>
<li><strong>Partial observability</strong>: Images tend to be a partial observation of the underlying state.</li>
</ul>
<p>Remember that states are the underlying structure which fully describe an instance of the environment, while observations are only what the agent perceives.\<br>
<strong>Example:</strong> in an Atari game, the observation is the screen output, and the state can be summarized in less dimensions by the position of the key elements.</p>
<p><strong>Idea:</strong> Can we learn separately $$p(o_t \mid s_t )$$ and $$p(s_{t+1} \mid s_t, a_t)$$?</p>
<ul>
<li>$$p(o_t \mid s_t )$$ is <strong>high-dimensional</strong> but <strong>static</strong>: given a state you can get its observation independent of the environment evolution.</li>
<li>$$p(s_{t+1} \mid s_t, a_t)$$: Is low-**dimensional** but **dynamic**: It models the environment transitions.</li>
</ul>
<h3 id=latent-space-models>Latent space models</h3>
<p>{% include end-row.html %}
{% include start-row.html %}</p>
<p>We need to learn models for:</p>
<ul>
<li><strong>Observation model:</strong> $$p(o_t \mid s_t )$$: To convert from states to observations.</li>
</ul>
<ul>
<li><strong>Dynamics model:</strong> $$p(s_{t+1} \mid s_t, a_t )$$: To know how the transitions work.</li>
<li><strong>Reward model:</strong> $$p(r_t \mid s_t, a_t )$$: To plan for maximum reward.</li>
</ul>
<p>{% include annotation.html %}
The state structure is not given to us, they are latent variables (helper random variables which are not observed but rather inferred from the observed variables).
{% include end-row.html %}
{% include start-row.html %}</p>
<h4 id=training-latent-space-models>Training Latent space models</h4>
<p>Given a dataset of trajectories: $$\mathcal{D} = {(s_{t+1} \mid s_t, a_t)_i}$$ we can train a **fully observed** ($$s_t = o_t$$) model maximizing over its parameters:</p>
<p>\begin{equation}
max_{\phi} \frac{1}{N} \sum_i \sum_t \log p_{\phi} (s_{t+1, i} \mid s_{t, i}, a_{t, i})
\end{equation}</p>
<p>For a <strong>latent space models</strong>, we do not know what $$s_t$$ is, so we need to take an expectation of it given the observed trajectory. Moreover, we also need to learn our observation model:</p>
<p>\begin{equation}
\label{eq:latent_learning}
max_{\phi} \frac{1}{N} \sum_i \sum_t
E_{(s, s_{t+1})\sim p(s_t, s_{t+1} \mid o_{1:T}, a_{1:T})} \left[
\log p_{\phi} (s_{t+1, i} \mid s_{t, i}, a_{t, i}) + \log p_{\phi} (o_{t, i} \mid s_{t, i})
\right]
\end{equation}</p>
<p>We can learn this approximation of the posterior in different ways:</p>
<ul>
<li><strong>Encoder</strong>: $$q_\psi (s_t \mid o_{1:t}, a_{1:t})$$: The most often used as leverages both next approximations.</li>
<li><strong>Full smoothing operator</strong>: $$q_\psi (s_t, s{t+1} \mid o_{1:T}, a_{1:T})$$ Which considers the whole trajectory. It is the most accurate but the most complicated to implement.</li>
<li><strong>Single-step encoder</strong>: $$q_\psi (s_t \mid o_t)$$: Its the simplest but the least accurate.</li>
</ul>
<p>In this lecture we will focus on <strong>single-step encoders</strong>:\<br>
We can compute the expectation in Eq. \ref{eq:latent_learning} by doing:
$$s_t \sim q_\psi (s_t \mid o_t), s_{t+1} \sim q_\psi(s_{t+1} \mid o_{t+1})$$.\<br>
In the special case where $$q_\psi (s_t \mid o_t)$$ is **deterministic** we can encode it with an ANN which given an observation returns the single most probable state: $$s_t = g_\psi (o_t)$$. Since it is deterministic, the expectation in Eq. \ref{eq:latent_learning} can be re-written substituting the random variable by its value:</p>
<p>\begin{equation}
\label{eq:latent_learning_det}
max_{\phi, \psi} \frac{1}{N} \sum_i \sum_t
\log p_{\phi} (g_\psi (o_{t+1, i}) \mid g_\psi (o_{t, i}), a_{t, i}) + \log p_{\phi} (o_{t, i} \mid g_\psi (o_{t, i}))
\end{equation}</p>
<p>Everything is differentiable so this can be trained using backprop. In addition, we can append our reward model as:</p>
<p>\begin{equation}
\label{eq:latent_learning_det_reward}
max_{\phi, \psi} \frac{1}{N} \sum_i \sum_t
\log p_{\phi} (g_\psi (o_{t+1, i}) \mid g_\psi (o_{t, i}), a_{t, i}) + \log p_{\phi} (o_{t, i} \mid g_\psi (o_{t, i)}) + \log p_\phi (r_{t,i} \mid g_\psi (o_{t,i}))
\end{equation}</p>
<p>In this order, we have the <strong>latent space dynamics</strong>, the <strong>observation reconstruction</strong> and the <strong>reward model</strong>. The previous model-based algorithm can be adapted as:</p>
<p>{% include end-row.html %}
{% include start-row.html %}</p>
<p>{% include figure.html url="/_rl/lecture_11/latent_space.png"%}</p>
<p>{% include annotation.html %}
How is this related to Hidden Markov Models (HMM) and Kalman filters (or Linear Quadratic Estimation, LQE)?</p>
<ul>
<li>All three rely on the same structure of learning a latent space given observations.</li>
<li><strong>HMM</strong> has states and observations which are all discrete (usually uses tabular representations).</li>
<li><strong>Kalman Filters</strong> has states and observations which are all continuous and uses linear Gaussian representations for everything.</li>
<li><strong>Latent space RL models</strong>: Can deal with much bigger spaces such as images thanks to ANNs.
{% include end-row.html %}
{% include start-row.html %}</li>
</ul>
<h3 id=observations-model>Observations model</h3>
<p>Do we really need to learn an embedding to get the underlying states?\<br>
<strong>Observations model</strong> directly learn $$p(o_{t+1} \mid o_t, a_t)$$. Usually combining CNNs with RNNs in a kind of video-prediction model achieves good results.</p>
<p>{% include end-row.html %}</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
0001-01-01
</span>
</p>
</div>
<div class=post-reward>
<input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>Reward</label>
<div class=qr-code>
<label class=qr-code-image for=reward>
<img class=image src=/img/reward/wechat.png>
<span>wechat</span>
</label>
<label class=qr-code-image for=reward>
<img class=image src=/img/reward/alipay.png>
<span>alipay</span>
</label>
</div>
</div><footer class=post-footer>
<nav class=post-nav>
<a class=prev href=/post/cs285_chapter12/lecture12/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Model-based Policy Learning</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/cs285_chapter16/lecture16/>
<span class="next-text nav-default">Transfer and multi-task RL</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>