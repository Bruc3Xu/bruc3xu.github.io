<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>Distributed RL - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/distributed_rl/distributed_rl/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="Distributed RL">
<meta property="og:description" content>
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/distributed_rl/distributed_rl/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-10-21T09:34:56+08:00">
<meta property="article:modified_time" content="2020-10-30T09:34:56+08:00">
<meta itemprop=name content="Distributed RL">
<meta itemprop=description content><meta itemprop=datePublished content="2020-10-21T09:34:56+08:00">
<meta itemprop=dateModified content="2020-10-30T09:34:56+08:00">
<meta itemprop=wordCount content="785">
<meta itemprop=keywords content="reinforcement learning,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Distributed RL">
<meta name=twitter:description content><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>Distributed RL</h1>
<div class=post-meta>
<span class=post-time> 2020-10-21 </span>
<div class=post-category>
<a href=/categories/cs285/> cs285 </a>
</div>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li><a href=#history-of-large-scale-distributed-rl>History of large scale distributed RL</a>
<ul>
<li><a href=#2013-original-dqnhttpsarxivorgabs13125602>2013. Original <a href=https://arxiv.org/abs/1312.5602>DQN</a></a></li>
<li><a href=#2015-general-reinforcement-learning-architecture-gorilahttpsarxivorgabs150704296>2015. General Reinforcement Learning Architecture (<a href=https://arxiv.org/abs/1507.04296>GORILA</a>)</a></li>
<li><a href=#2016-asynchronous-advantage-actor-critic-a3chttpsarxivorgpdf160201783pdf>2016. Asynchronous Advantage Actor Critic (<a href=https://arxiv.org/pdf/1602.01783.pdf>A3C</a>)</a></li>
<li><a href=#2017-importance-weighted-actor-learner-architectures-impalahttpsarxivorgabs180201561>2017. Importance Weighted Actor-Learner Architectures (<a href=https://arxiv.org/abs/1802.01561>IMPALA</a>)</a></li>
<li><a href=#2018-ape-xhttpsarxivorgabs180300933--r2d2httpsopenreviewnetpdfidr1lytjaqyx>2018. <a href=https://arxiv.org/abs/1803.00933>Ape-X</a> / <a href="https://openreview.net/pdf?id=r1lyTjAqYX">R2D2</a></a></li>
<li><a href=#2019-using-expert-demonstrations-r2d3httpsarxivorgabs190901387>2019. Using expert demonstrations <a href=https://arxiv.org/abs/1909.01387>R2D3</a></a></li>
<li><a href=#others>Others:</a>
<ul>
<li><a href=#qt-opthttpsarxivorgpdf180610293pdf><a href=https://arxiv.org/pdf/1806.10293.pdf>QT-Opt</a></a></li>
<li><a href=#evolution-strategieshttpsarxivorgabs170303864><a href=https://arxiv.org/abs/1703.03864>Evolution Strategies</a></a></li>
<li><a href=#population-based-traininghttpsdeepmindcomblogarticlepopulation-based-training-neural-networks><a href=https://deepmind.com/blog/article/population-based-training-neural-networks>Population-based Training</a></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>分布式强化学习算法可以大幅提升采样效率，加速学习速度，对于on-policy算法一定程度也能减少方差。</p>
<h1 id=history-of-large-scale-distributed-rl>History of large scale distributed RL</h1>
<h2 id=2013-original-dqnhttpsarxivorgabs13125602>2013. Original <a href=https://arxiv.org/abs/1312.5602>DQN</a></h2>
<p>2013年DeepMind实现的DQN算法。</p>
<p><img src=/post/distributed_rl/dqn.png alt></p>
<h2 id=2015-general-reinforcement-learning-architecture-gorilahttpsarxivorgabs150704296>2015. General Reinforcement Learning Architecture (<a href=https://arxiv.org/abs/1507.04296>GORILA</a>)</h2>
<p>可以分为4个可重复的部分，运行在不同的节点上：</p>
<ul>
<li><strong>replay buffer/memory</strong>: 存储transition$(s, a, r, s^\prime)$</li>
<li><strong>learner</strong>: 从replay memory拉取数据更新Q网络</li>
<li><strong>actor</strong>: 拉取网络参数，与环境交互得到$(s, a, r, s^\prime)$ 并存入memory buffer</li>
<li>The <strong>parameter server</strong>: 不断更新保存Q网络的参数</li>
</ul>
<p><img src=/post/distributed_rl/gorila.png alt></p>
<p><strong>Bottleneck</strong>:
actor拉取频率过快影响采样速度。</p>
<h2 id=2016-asynchronous-advantage-actor-critic-a3chttpsarxivorgpdf160201783pdf>2016. Asynchronous Advantage Actor Critic (<a href=https://arxiv.org/pdf/1602.01783.pdf>A3C</a>)</h2>
<p><img src=/post/distributed_rl/a3c.png alt>
所有worker在一台机器上，并通过共享内存分享网络权重。</p>
<p>每个worker采样更新自身的网络，计算梯度并上传。</p>
<p>每个worker利用反馈的总梯度更新网络。</p>
<p>优点：</p>
<ul>
<li>没有网络开销</li>
<li>异步更新，采样更快</li>
</ul>
<p>缺点：</p>
<ul>
<li>policy lag：不同worker的网络参数差距过大，总的梯度计算就会不准确。</li>
</ul>
<h2 id=2017-importance-weighted-actor-learner-architectures-impalahttpsarxivorgabs180201561>2017. Importance Weighted Actor-Learner Architectures (<a href=https://arxiv.org/abs/1802.01561>IMPALA</a>)</h2>
<p>主要包括：</p>
<ul>
<li><strong>Learners</strong>: 并行梯度更新算法，需要等待收集一个batch的样本学习</li>
<li><strong>Actors</strong>: 独立于learner，互相不关联，异步更新。</li>
</ul>
<p><strong>Solution:</strong> V-trace机制纠正了新旧策略之间的差异，解决了policy lag问题。<a href=https://arxiv.org/abs/1802.01561>paper</a></p>
<p><img src=/post/distributed_rl/impala.png alt></p>
<h2 id=2018-ape-xhttpsarxivorgabs180300933--r2d2httpsopenreviewnetpdfidr1lytjaqyx>2018. <a href=https://arxiv.org/abs/1803.00933>Ape-X</a> / <a href="https://openreview.net/pdf?id=r1lyTjAqYX">R2D2</a></h2>
<p>与GORILA类似，replay buffer，actors和learner位于不同的节点，actors异步更新。不同的是提出了
<strong>distributed prioritization</strong>，认为不同样本的学习优先级不同，可以适用于分布式的样本。</p>
<p><img src=/post/distributed_rl/apex.png alt></p>
<p>R2D2 (Recurrent Ape-X algorithm)使用了LSTM结构。</p>
<h2 id=2019-using-expert-demonstrations-r2d3httpsarxivorgabs190901387>2019. Using expert demonstrations <a href=https://arxiv.org/abs/1909.01387>R2D3</a></h2>
<p>同时使用专家经验和策略与环境交互数据，并分配不同的比重。</p>
<p><img src=/post/distributed_rl/r2d3.png alt></p>
<h2 id=others>Others:</h2>
<h3 id=qt-opthttpsarxivorgpdf180610293pdf><a href=https://arxiv.org/pdf/1806.10293.pdf>QT-Opt</a></h3>
<p>This distributed architecture main feature is that it interfaces with the real world.
Moreover, its weight update happens asynchronously.
Meaning it can be easily heavily parallelized into multiple cores (can be <strong>independently scaled</strong>).
It was designed for robotic grasping, using a setup of 7 robots creating samples.</p>
<p><img src=/post/distributed_rl/qt_opt.png alt></p>
<h3 id=evolution-strategieshttpsarxivorgabs170303864><a href=https://arxiv.org/abs/1703.03864>Evolution Strategies</a></h3>
<p>Gradient-free approach by OpenAI.
Essentially uses an evolutionary algorithm on the ANN weights.
It works by having multiple instances of the network where it applies some random noise.
The idea is then to run the policies and perform a weighted average parameter update based on the performance of each policy.</p>
<p><img src=/post/distributed_rl/evolution.png alt></p>
<h3 id=population-based-traininghttpsdeepmindcomblogarticlepopulation-based-training-neural-networks><a href=https://deepmind.com/blog/article/population-based-training-neural-networks>Population-based Training</a></h3>
<p>Technique for hyperparameter optimization.
Merges the idea of a grid search but instead of the networks training independently, it uses information from the rest of the population to refine the hyperparameters and direct computational resources to models which show promise.</p>
<p>Using this technique one can improve the performance of any hyperparam-dependent algorithm.</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2020-10-30
</span>
</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/soft_actor_critic/soft-actor-critic/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Soft Actor Critic</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/python_dataclass/>
<span class="next-text nav-default">Python Dataclass</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>