<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes lecture 12: Model-based Policy Learning - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_lecture12/lecture12/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes lecture 12: Model-based Policy Learning">
<meta property="og:description" content>
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_lecture12/lecture12/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-10-15T09:34:56+08:00">
<meta property="article:modified_time" content="2020-10-30T09:34:56+08:00">
<meta itemprop=name content="cs285 DRL notes lecture 12: Model-based Policy Learning">
<meta itemprop=description content><meta itemprop=datePublished content="2020-10-15T09:34:56+08:00">
<meta itemprop=dateModified content="2020-10-30T09:34:56+08:00">
<meta itemprop=wordCount content="1418">
<meta itemprop=keywords content="reinforcement learning,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes lecture 12: Model-based Policy Learning">
<meta name=twitter:description content><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes lecture 12: Model-based Policy Learning</h1>
<div class=post-meta>
<span class=post-time> 2020-10-15 </span>
<div class=post-category>
<a href=/categories/cs285/> cs285 </a>
</div>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#naive-approach>Naive approach</a>
<ul>
<li><a href=#problems>Problems:</a></li>
<li><a href=#solutions>Solutions:</a></li>
</ul>
</li>
<li><a href=#model-free>Model-free</a>
<ul>
<li></li>
<li><a href=#dyna-algorithm>Dyna Algorithm</a></li>
<li><a href=#generalized-dyna-style-algorithms>Generalized Dyna-style Algorithms</a></li>
</ul>
</li>
<li><a href=#local-policies>Local policies</a>
<ul>
<li></li>
</ul>
</li>
<li><a href=#guided-policy-search>Guided policy search</a></li>
<li><a href=#distillation-in-supervised-learning>Distillation in Supervised Learning</a></li>
<li><a href=#policy-distillation>Policy Distillation</a></li>
<li><a href=#divide-and-conquer-rl>Divide and Conquer RL</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>In the previous lecture: <a href=/lectures/lecture11>Model-based RL</a>, we where planning trajectories (<strong>stochastic open-loop</strong>), by maximizing the expected reward over a sequence of actions:
$$
a_1,&mldr;,a_T = \arg \max_{a_1,&mldr;,a_T} E \left[ \sum_t r(s_t, a_t) \mid a_1,&mldr;, a_T \right]
$$</p>
<p>Now we will build a policies capable of adapting to the situation (<strong>stochastic closed-loop</strong>), by maximizing a reward expectation:</p>
<p>$$
\pi = \arg \max_{\pi} E_{\tau \sim p(\tau)} \left[ \sum_t r(s_t, a_t) \right]
$$</p>
<h2 id=naive-approach>Naive approach</h2>
<p>Backprop the $s_{t+1}$ error into our env model prediction and the $r_t$ error into our policy:</p>
<p>![](/post/cs285_lecture12/naive_comp_graph.png" description=&ldquo;Backprop though time computational graph.)</p>
<p>The pseudo-code would then be:</p>
<p><img src=/post/cs285_lecture12/naive_code.png alt></p>
<h3 id=problems>Problems:</h3>
<ul>
<li>Propagating gradient through long trajectories often causes <strong>vanishing or exploding gradient</strong> issues (depending on the eigenvalues of the models Jacobians). But unlike LSTMs we cannot choose simpler dynamics, they are chosen by the environment.</li>
<li><strong>Ill conditioning</strong> due to high parameter sensitivity. Same as <a href=/lectures/lecture10>shooting methods</a>, first actions affect trajectory much more significantly than last ones. But no dynamic programming (like LQR) can be applied since policy params couple all the time steps.</li>
</ul>
<h3 id=solutions>Solutions:</h3>
<p>General idea of the solutions, developed further in subsequent sections.</p>
<ul>
<li>
<p><strong>Model-based acceleration</strong>: Use model-free RL algorithms (which are derivative free), using our learned model to generate synthetic samples. Even though it seems counter-productive it works well.</p>
</li>
<li>
<p><strong>Use simple policies</strong> (rather than ANNs). This allows us to use second-order optimization (aka Newton Method) which mitigates the mentioned problems. Some applications are:</p>
<ul>
<li>Linear Quadratic Regulator with Fitted Local Models (LQR-FLM).</li>
<li>Train <strong>local</strong> policies to solve simpler tasks.</li>
<li>Combine them into <strong>global</strong> policies via supervised learning.</li>
</ul>
</li>
</ul>
<h2 id=model-free>Model-free</h2>
<p>We have two equivalent options to approximate $\nabla_{\theta} J(\theta)$:</p>
<h4 id=policy-gradient>Policy gradient:</h4>
<p>Avoids backprop through time as it treats the derivation of an expectation as the derivation of sums of its states probabilities:</p>
<p>$$
\nabla_{\theta} J(\theta) \simeq
\frac{1}{N} \sum_i^N \sum_t \nabla_{\theta} \log \pi_{\theta} (a_{i, t} \mid s_{i, t})
\hat Q^\pi (s_t^i, a_t^i)
$$</p>
<ul>
<li>Has a <strong>high variance</strong>, but can be mitigated by training with more samples: Thats where using a learned model cheaper than real env. to generate multiple synthetic samples helps.</li>
</ul>
<h4 id=path-wise-backprop-gradient>Path-wise backprop gradient:</h4>
<p>$$
\nabla_{\theta} J(\theta) =
\sum_t \frac{dr_t}{ds_t}
\prod_{t^{\prime}}
\frac{ds_{t^{\prime}}}{da_{t^{\prime} - 1}}
\frac{da_{t^{\prime} - 1}}{ds_{t^{\prime} - 1}}
$$</p>
<ul>
<li>It is very <strong>ill-conditioned</strong> (unstable) since applying the chain rule to numerous successive elements of the trajectory results in the product of many Jacobians.</li>
<li>If using long trajectories, our synthetic model might give erroneous estimations.</li>
<li>Has a <strong>lower variance</strong>.</li>
</ul>
<h3 id=dyna-algorithm>Dyna Algorithm</h3>
<p>Online Q-learning algorithm performing model-free RL with a model to help compute future expectations.</p>
<p><img src=/post/cs285_lecture12/dyna.png alt></p>
<p>{% include annotation.html %}
We use our learned synthetic model to make better estimations of future rewards.</p>
<h3 id=generalized-dyna-style-algorithms>Generalized Dyna-style Algorithms</h3>
<p>Online Q-learning algorithm performing model-free RL with a model to help compute future expectations.</p>
<p>![](/post/cs285_lecture12/dyna-style.png&rdquo; description=&ldquo;Generalyzed Dyna-style algorithms pseudocode.)</p>
<p>![](/post/cs285_lecture12/gen_dyna_idea.png&rdquo; description=&ldquo;Generalyzed Dyna approach. Black arrows are the real-world traversed trajectories. Tan points the samples from which to generate synthetic trajectories. Red arrows the simulated trajectories using our learned transition model.)</p>
<p><strong>Pros</strong>:</p>
<ul>
<li><span style=color:green>We augment the training states by generating new samples (this reduces variance).</span></li>
<li><span style=color:green>Env. model doesn&rsquo;t need to be super good since we only use it to simulate few steps close to real states.</span></li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li><span style=color:red>Initial env. model might be very bad and mess up the policy approximator.</span></li>
<li><span style=color:red>Learning a decent model of the environment in some cases might be harder than learning the Q function.</span></li>
</ul>
<h2 id=local-policies>Local policies</h2>
<p>In the standard RL setup, the main thing we lack to use LQR is: $\frac{df}{dx_t}$, $\frac{df}{du_t}$ (control notation).</p>
<p><strong>Idea</strong>: Fit $\frac{df}{dx_t}$, $\frac{df}{du_t}$ around taken trajectories. By using LQR we have a linear feedback controller which can be executed in the real world.</p>
<ol>
<li>Run policy $\pi$ on robot, to collect trajectories: $\mathcal{D} = { \tau_i }$.</li>
<li>Fit $A_t \simeq \frac{df}{dx_t}, B_t \simeq \frac{df}{du_t}$ in a linear synthetic dynamics model: $f(x_t, u_t) \simeq A_t x_t + B_t u_t$ s.t. $p(x_{t+1} \mid x_t, u_t) \sim \mathcal{N} (f(x_t, u_t), \Sigma)$.\<br>
**Reminder** from <a href=/lectures/lecture10>LQR lecture</a>: $\Sigma$ does not affect the answer, so no need to fit it.</li>
<li>Improve controller and repeat.</li>
</ol>
<h4 id=controller-step-1>Controller (step 1.)</h4>
<p><strong>iLQR</strong> produces: $\hat x_t, \hat u_t K_t, k_t$ s.t.
$u_t = K_t (x_t - \hat x_t) + k_t + \hat u_t$. but what controller should we execute?</p>
<ul>
<li>$p(u_t \mid x_t) = \delta (u_t = \hat u_t)$ doesn&rsquo;t correct for deviations or drift.</li>
<li>$p(u_t \mid x_t) = \delta (u_t = K_t (x_t - \hat x_t) + k_t + \hat u_t)$ might be so good that it doesn&rsquo;t produce different enough trajectories to fit a decent env. model (you cannot do linear regression if all your points look the same).</li>
<li>$p(u_t \mid x_t) = \mathcal{N} (u_t = K_t (x_t - \hat x_t) + k_t + \hat u_t, \Sigma_t)$ adds the needed noise so not all trajectories are the same. A good choice is $\Sigma = Q_{u_t, u_t}^{-1}$ (Q matrix from LQR method).\<br>
**OBS**: $Q_{u_t, u_t}$ matrix of LQR method models the local curvature of $Q$ function.
If it&rsquo;s very shallow, you can afford to be very random. Otherwise, it means that action heavily influences the outcome and you shouldn&rsquo;t introduce that much variance.</li>
</ul>
<h4 id=fitting-dynamics-step-2>Fitting dynamics (step 2.)</h4>
<p><strong>Ideas</strong>:</p>
<ul>
<li>Fit $A_t, B_t$ matrices of $p(x_{t+1} \mid x_t, u_t)$ at each time-step using **linear regression** with the ${x_t , u_t, x_{t+1}}$ point received.\<br>
**Problem**: Linear regression scales with the dimensionality of the state. Very high-dim states need way more samples.</li>
<li>Fit $p(x_{t+1} \mid x_t, u_t)$ using **Bayesian linear regression** with a prior given by any global model (GP, ANN, GMM&mldr;). This improves performance with less samples.</li>
</ul>
<p><strong>Problem</strong>:
Most real problems are not linear: Linear approximations are only good close to the traversed trajectories.</p>
<p><strong>Solution</strong>:
Try to keep new trajectory probability distribution $p(\tau)$ &ldquo;close&rdquo; to the old one $\hat p(\tau)$. If the distribution is close, the dynamics will be as well. By close we mean small KL divergence: $D_{KL} (p(\tau) || \hat p(\tau)) &lt; \epsilon$. Turns out it is very easy to do for LQR models by just modifying the reward function of the new controller to add the log-probability of the old one. More in this <a href=https://papers.nips.cc/paper/5444-learning-neural-network-policies-with-guided-policy-search-under-unknown-dynamics>paper</a>.</p>
<p>Still, the learned policies will be only <strong>local</strong>! We need a way to combine them:</p>
<h2 id=guided-policy-search>Guided policy search</h2>
<p>Use a weaker learner (e.g. model-based local-policy learner) to guide the learning of a more complex global policy (e.g. ANN).</p>
<p>For instance, if we have an environment with different possible starting states, we can cluster them and train a separate LQR controller for each cluster (each one being only responsible for a narrow region of the state-space: <strong>trajectory-centric RL</strong>). Then we can use it to learn a single ANN policy (using supervised learning) which learns starting from any state.</p>
<p><strong>Problem</strong>: The learned controllers behavior might not be reproducible by a single ANN. They all have different local optima and an ANN can only have one.</p>
<p><strong>Solution</strong>: After training the ANN, go back and modify the weak learners rewards to try to mimic the ANN as well.This way, after re-training, the global optima should be found:</p>
<p>![](/post/cs285_lecture12/guided_pol.png&rdquo; description=&ldquo;Guided policy search algorithm. $\pi_{theta}$ is the global ANN-modelled policy. $\lambda$ is the Lagrange multiplier and the sign of the equation in step 3 should be negative.)</p>
<p>{% include annotation.html %}
More on this <a href=https://arxiv.org/abs/1504.00702>paper</a>.</p>
<p>This idea of combining local policies and a single global policy ANN can be used in other settings beyond model-based RL (it also works well on model-free RL).</p>
<h2 id=distillation-in-supervised-learning>Distillation in Supervised Learning</h2>
<p><strong>Distillation</strong>: Given an ensemble of weaker models, we can train a single one that matches their performance by using each model output of the ensemble as a &ldquo;soft&rdquo; target (e.g. applying Softmax over them). The intuition is that the ensemble adds knowledge to the otherwise hard labels, such as: which ones can be confusing.</p>
<p>{% include annotation.html %}
More on this <a href=https://arxiv.org/abs/1503.02531>paper</a></p>
<h2 id=policy-distillation>Policy Distillation</h2>
<p>Distillation concept can be brought to RL.
For instance in this <a href=https://arxiv.org/abs/1511.06342>paper</a> they train an agent to play all Atari games.
They train a different policies to play each of the games and then use supervised learning to train a single policy which plays all of them.
This technique seems to be easier than multi-task RL training.</p>
<p>{% include annotation.html %}
This is analogous to <strong>Guided policy search</strong> but for multi-task learning</p>
<h2 id=divide-and-conquer-rl>Divide and Conquer RL</h2>
<p>We can use the loop presented in <strong>Guided policy search</strong> also in this setting to improve the specific policies using the global policy:</p>
<p>![](/post/cs285_lecture12/div_conq_rl.png&rdquo; description=&ldquo;Divide and conquer RL algorithm. $\pi_{theta}$ is the global ANN-modelled policy. Now $\pi_{phi_i}$ are also modelled by ANNs. $\lambda$ is the Lagrange multiplier and the sign of the equation in step 3 should be negative. $x \equiv s$, $u \equiv a$.)</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2020-10-30
</span>
</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/cs285_lecture20/lecture20/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">cs285 DRL notes lecture 20: Inverse Reinforcement Learning (IRL)</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/cs285_lecture11/lecture11/>
<span class="next-text nav-default">cs285 DRL notes lecture 11: Model-based RL</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>