<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes chapter 6: Deep RL with Q-Functions - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_chapter6/chapter6/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes chapter 6: Deep RL with Q-Functions">
<meta property="og:description" content>
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_chapter6/chapter6/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-09-18T15:46:38+08:00">
<meta property="article:modified_time" content="2020-09-18T15:46:38+08:00">
<meta itemprop=name content="cs285 DRL notes chapter 6: Deep RL with Q-Functions">
<meta itemprop=description content><meta itemprop=datePublished content="2020-09-18T15:46:38+08:00">
<meta itemprop=dateModified content="2020-09-18T15:46:38+08:00">
<meta itemprop=wordCount content="2283">
<meta itemprop=keywords content><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes chapter 6: Deep RL with Q-Functions">
<meta name=twitter:description content><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes chapter 6: Deep RL with Q-Functions</h1>
<div class=post-meta>
<span class=post-time> 2020-09-18 </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#q-learning的问题>Q-Learning的问题</a>
<ul>
<li><a href=#q-learning-is-not-gd>Q-learning is not GD</a></li>
<li><a href=#one-step-gradient>one-step gradient</a></li>
<li><a href=#moving-targets>moving targets</a></li>
<li><a href=#correlate-samples>correlate samples</a></li>
</ul>
</li>
<li><a href=#replay-buffers>replay buffers</a></li>
<li><a href=#target-network>target network</a></li>
<li><a href=#double-q-learning>Double Q-Learning</a></li>
<li><a href=#multi-step-returns>Multi Step Returns</a></li>
<li><a href=#q-learning-with-continuous-actions>Q Learning with Continuous Actions</a>
<ul>
<li><a href=#stochastic-optimization>Stochastic Optimization</a></li>
<li><a href=#easily-maximizable-q-functions>Easily Maximizable Q Functions</a></li>
<li><a href=#ddpg-deep-deterministic-policy-gradient>DDPG (Deep Deterministic Policy Gradient)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<ul>
<li>fitted q iteration算法：当前策略收集整个数据集，然后对Q函数进行多次回归近似，接下来收集新的数据集循环这一过程。</li>
<li>Q-Learning（online q iteration）：一边收集数据，一边进行学习。</li>
</ul>
<h2 id=q-learning的问题>Q-Learning的问题</h2>
<h3 id=q-learning-is-not-gd>Q-learning is not GD</h3>
<p>Q-learning不是梯度下降，公式</p>
<p>$$
\begin{aligned}
y_i&=r(s_i,a_i)+\gamma max_{a'}Q_\phi(s_i',a_i')\\<br>
\phi&\leftarrow \phi-\alpha{dQ_\phi\over d\phi}(s_i,a_i)(Q_\phi(s_i,a_i)-y_i)<br>
\end{aligned}
$$</p>
<p>其中$y_i$与参数$\phi$相关，但没有梯度计算，因而不保证收敛。</p>
<h3 id=one-step-gradient>one-step gradient</h3>
<p>Q-Learning使用一个样本来更新梯度，更新非常不稳定，使用批次更新最优。</p>
<h3 id=moving-targets>moving targets</h3>
<p>目标值因为与神经网络参数相关，每次神经网络更新，目标值也会随之变动，严重影响收敛。</p>
<h3 id=correlate-samples>correlate samples</h3>
<p>我们使用神经网路来近似Q函数，而神经网络学习数据一般要求独立同分布（i.i.d），但是在rl的采样中，序列决策问题的数据来源于与环境的相邻交互，数据之间往往是具有相关性的，即可能状态相似。神经网络会在局部状态过拟合，当学习新的状态时，遗忘旧的内容。</p>
<h2 id=replay-buffers>replay buffers</h2>
<p>Replay Buffer是一个或者多个智能体采样得到样本的合集，会将所有样本都存储起来，直至容量上限。
当使用Q-Learning算法学习时，从其中随机拿出部分样本来进行神经网络训练。
Replay Buffer解决了上面提到的两个问题：one-step gradient和correlate samples。</p>
<p>使用Replay Buffer的Q-Learning算法如下</p>
<hr>
<p>使用某种策略收集transition${(s_i,a_i,s'_i,r_i)}$并存储到replay buffer $\mathcal{B}$<br>
For $K$ times<br>
$\quad \text{获取样本批次}(s_i,a_i,s'_i,r_i)\leftarrow \mathcal{B}$<br>
$\quad y_i\leftarrow r(s_i,a_i) + \gamma \max_{a'_i}Q_\phi(s'_i,a'_i)$<br>
$\quad \phi \leftarrow \phi-\alpha\Sigma_i\frac{dQ_\phi}{d\phi}(s_i,a_i)(Q_\phi(s_i,a_i) - y_i)$</p>
<hr>
<p>我们需要关心的是</p>
<ul>
<li>replay buffer的填充频率</li>
<li>存储transition的数量</li>
<li>采样的方式（均匀、权重等）</li>
</ul>
<h2 id=target-network>target network</h2>
<p>我们希望目标值能够固定，一种方式是使用Q神经网络，固定其参数，用来计算目标值。目标网络会在神经网络更新后每隔一段时间进行更新，然后重复这一过程。
<img src=/post/cs285_chapter6/qlearning_with_target.png alt></p>
<p>当N=1，K=1，是上述算法就成为了经典DQN算法。
<img src=/post/cs285_chapter6/dqn.png alt></p>
<p>为了避免Q函数和目标函数的lag变化过大，可以使用soft update
$$\phi '=\tau\phi ' + (1-\tau)\phi \quad \ \tau=0.999$$</p>
<h2 id=double-q-learning>Double Q-Learning</h2>
<p>Q-Learning算法存在<strong>overestimation</strong>问题，目标值
$$
y = r + \gamma \max_{a'}Q_{\phi'}(s', a')
$$</p>
<p>对于$n$个随机变量$X_1$, .. $X_n$,
$$
E\left[\max(X_1,\ &mldr;,\ X_n)\right] \ge max(E[X_1],\ &mldr;,\ E[X_n])
$$
因此，当我们对目标值y取$\max$操作时，会不可避免高估Q值。 对于$Q_{\phi'}$的$\max$操作等同于
对取得最大$Q_{\phi'}$值的动作做$\arg\max$操作。</p>
<p>$$
\max_{a'}Q_{\phi'}(s', a') = Q_{\phi}(s', \arg\max_{a'}Q_{\phi'}(s', a'))
$$</p>
<p>这个操作可以描述为两个过程，根据$Q_{\phi'}$去选择一个最佳的action，然后得到这个最佳action的Q值。over-estimation因为这个选到的aciton必然会导致得到的Q值最大。故而只要破坏这种相关性，这个问题就可以得到一定程度的解决.</p>
<p>Double Q-learning在这两个步骤中分别使用两个Q神经网络作为estimator，从而缓解这个问题。</p>
<p>$$
y = r + \gamma Q_{\phi'}(s', \arg\max_{a'}Q_{\phi}(s', a'))
$$</p>
<p>选择action时对$Q_{\phi}$做$\arg\max$操作，然后基于$Q_{\phi'}$求对应的Q值。考虑到上文中已经提出对应的target network，可以仍然使用target network来估计Q值，选择动作时使用当前的网络$Q_{\phi}$。</p>
<h2 id=multi-step-returns>Multi Step Returns</h2>
<p>在actor-critic算法中，策略梯度目标值的计算有几种方式
$$
\begin{aligned}
Q&=\mathbb{E}[r_0+\gamma r_1+&mldr;+\gamma^{n}r_n]\quad\text{Monte-Carlo} \\<br>
&=\mathbb{E}[r_0+\gamma V^\pi(s_1)]\quad \text{1 step TD} \\<br>
&=\mathbb{E}[r_0+\gamma r_1+\gamma^2 V^\pi(s_2)]\quad \text{2 step TD} \\<br>
&=\mathbb{E}[r_0+\gamma r_1+&mldr;+\gamma^{n}r_n+\gamma^n V^\pi(s_n)]\quad \text{n step TD} \\<br>
\end{aligned}
$$</p>
<p>Monte-Carlo：无偏差，高方差；
n-step returns：n越大，偏差越小，方差越大。</p>
<p>与之类似，我们构造Q-Learning的<strong>multi-step returns</strong>
$$
y_t = \sum_{t'=t}^{t+N-1} \gamma^{t'-t} r_{t'} + \gamma^{N} \max_{a}Q_{\phi}(s_{t+N}, a)
$$
优缺点：</p>
<ul>
<li>当Q值不准确的时候偏差更小</li>
<li>学习更快，尤其在前期（前期Q值估计不准确，targets中reward项占据主要部分）</li>
<li>只有on-policy（N=1）才正确</li>
</ul>
<p>如何解决这个问题？</p>
<ol>
<li>最常见的方法就是忽略它们，在现在大部分使用n-steps的算法，包括GORILA、Ape-X，R2D2等都是这样操作的。</li>
<li>另外则是可以利用某些手段去cut掉这些trace，一般都是通过两个policy生成这个trace的probability比值是否超过某个阈值来判定，比较著名的就是Retrace以及基于它的ACER，Reacotr，IMPALA等。</li>
<li>使用重要性采样。</li>
</ol>
<h2 id=q-learning-with-continuous-actions>Q Learning with Continuous Actions</h2>
<p>连续动作空间在计算targets的时候会遇到问题，主要是关于动作的$\arg \max$操作无法实现。</p>
<h3 id=stochastic-optimization>Stochastic Optimization</h3>
<p>一个简单有效的方法是采样$N$个动作$a_1$, &mldr;, $a_N$并选择Q值最大的动作：
$$
\max_a Q_{\phi}(s, a) \approx max(Q_{\phi}(s, a_1),\ &mldr;,\ Q_{\phi}(s, a_N))
$$
这种方法不是很准确，但速度快并且可以并行采样。</p>
<p>更准确的方法有：</p>
<ul>
<li><strong>Cross Entropy Methods</strong>: 采集$N$个动作，利用最小化交叉熵来学习相应的策略分布，随着学习得到的策略越来越准确，得到的样本也越来越好。</li>
<li><strong>CMA-ES</strong>: 一种进化方法。</li>
</ul>
<h3 id=easily-maximizable-q-functions>Easily Maximizable Q Functions</h3>
<p>第二大类则是引入一些容易取max的Q-function。例如在NAF(Normalized Advantage Functions)中，将网络的输出分成三部分：</p>
<p><img src=/post/cs285_chapter6/NAF.png alt></p>
<p>此时max操作和argmax操作都可以用其中的head来表示：
$$
\arg\max_a Q_{\phi}(s, a) = \mu_{\phi}(s)
$$
$$
\max_a Q_{\phi}(s, a) = V_{\phi}(s)
$$
而一般的Q值则是也可以通过它们组合得到：</p>
<p>$$
Q_{\phi}(s, a) = -\frac{1}{2}(a - \mu_{\phi}(s))^T P_{\phi}(s)(a - \mu_{\phi}(s)) + V_{\phi}(s)
$$</p>
<p>这种方法的优点就是不对algorihtm本身作出改动，没有添加inner loop的计算量，效率保持一致。但是由于网络需要输出多个head，表达更多语意，会降低表达能力，需要更大网络。</p>
<h3 id=ddpg-deep-deterministic-policy-gradient>DDPG (Deep Deterministic Policy Gradient)</h3>
<p>训练另一个神经网络$\mu_{\theta}(s)$来近似$\arg\max_a Q_{\phi}(s, a)$。</p>
<p>$$
\theta \leftarrow \arg\max_{\theta} Q_{\phi}(s, \mu_{\theta}(s))
$$</p>
<p>$$
\frac{dQ_{\phi}}{d\theta} = \frac{da}{d\theta} \frac{dQ_{\phi}}{da}
$$</p>
<p><strong>DDPG</strong> 算法：</p>
<p><img src=/post/cs285_chapter6/ddpg.png alt></p>
</div>
<footer class=post-footer>
<nav class=post-nav>
<a class=next href=/post/cs285_chapter5/chapter5/>
<span class="next-text nav-default">cs285 DRL notes chapter 5: value function methods</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>