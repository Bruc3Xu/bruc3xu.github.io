<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on RuChen Xu's Blog</title><link>http://localhost:1313/post/</link><description>Recent content in Posts on RuChen Xu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 23 May 2021 09:20:09 +0800</lastBuildDate><atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Cpp编程规范</title><link>http://localhost:1313/post/cpp_code/</link><pubDate>Sun, 23 May 2021 09:20:09 +0800</pubDate><guid>http://localhost:1313/post/cpp_code/</guid><description>&lt;h1 id="代码风格检查">代码风格检查&lt;/h1>
&lt;h2 id="11-代码规范">1.1. 代码规范&lt;/h2>
&lt;p>尽量遵循 Google C++ Style Guide&lt;/p></description></item><item><title>使用gitflow模式开发</title><link>http://localhost:1313/post/%E4%BD%BF%E7%94%A8gitflow%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%8F%91/</link><pubDate>Sat, 24 Apr 2021 11:19:17 +0800</pubDate><guid>http://localhost:1313/post/%E4%BD%BF%E7%94%A8gitflow%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%8F%91/</guid><description>&lt;p>Gitflow工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅，非常适合用来管理大型项目的发布和维护&lt;/p></description></item><item><title>Soft Actor Critic</title><link>http://localhost:1313/post/soft_actor_critic/soft-actor-critic/</link><pubDate>Fri, 13 Nov 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/soft_actor_critic/soft-actor-critic/</guid><description>&lt;p>现有深度强化学习算法主要的问题有：&lt;/p>
&lt;ul>
&lt;li>采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。&lt;/li>
&lt;/ul></description></item><item><title>Distributed RL</title><link>http://localhost:1313/post/distributed_rl/distributed_rl/</link><pubDate>Wed, 21 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/distributed_rl/distributed_rl/</guid><description>&lt;p>分布式强化学习算法可以大幅提升采样效率，加速学习速度，对于on-policy算法一定程度也能减少方差。&lt;/p></description></item><item><title>Python Dataclass</title><link>http://localhost:1313/post/python_dataclass/</link><pubDate>Sat, 10 Oct 2020 13:59:38 +0800</pubDate><guid>http://localhost:1313/post/python_dataclass/</guid><description>&lt;p>dataclass是python3.7的新特性，是一个主要包含数据的类。&lt;/p></description></item><item><title>cs285 DRL notes lecture 10: Model-based Planning</title><link>http://localhost:1313/post/cs285_lecture10/lecture10/</link><pubDate>Tue, 29 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture10/lecture10/</guid><description>&lt;p>model-free强化学习忽略了状态转移概率$p(s_{t+1}|s_t,a_t)$，因为实际情况环境的模型往往无法获得或者学习。但也有一些例外：&lt;/p></description></item><item><title>cs285 DRL notes lecture 9: Advanced Policy Gradients</title><link>http://localhost:1313/post/cs285_lecture9/lecture9/</link><pubDate>Wed, 23 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture9/lecture9/</guid><description>&lt;p>本章会深入策略梯度算法，进一步学习
&lt;strong>Natural Policy Gradient&lt;/strong>, &lt;strong>Trust Region Policy Optimization&lt;/strong>, &lt;strong>Proximal Policy
Optimization&lt;/strong>等算法。&lt;/p></description></item><item><title>Git配置github和gitlab</title><link>http://localhost:1313/post/git%E9%85%8D%E7%BD%AEgithub%E5%92%8Cgitlab/</link><pubDate>Sun, 20 Sep 2020 11:11:27 +0800</pubDate><guid>http://localhost:1313/post/git%E9%85%8D%E7%BD%AEgithub%E5%92%8Cgitlab/</guid><description>&lt;p>在工作中，我们经常会遇到同时使用github、gitlab、自建git服务器等，下面看一下具体如何配置。&lt;/p></description></item><item><title>cs285 DRL notes lecture 8: Deep RL with Q-Functions</title><link>http://localhost:1313/post/cs285_lecture8/lecture8/</link><pubDate>Fri, 18 Sep 2020 15:46:38 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture8/lecture8/</guid><description>&lt;p>fitted q iteration与Q-Learning不同&lt;/p>
&lt;ul>
&lt;li>fitted q iteration算法：当前策略收集整个数据集，然后对Q函数进行多次回归近似，接下来收集新的数据集循环这一过程。&lt;/li>
&lt;li>Q-Learning（online q iteration）：一边收集数据，一边进行学习。&lt;/li>
&lt;/ul></description></item><item><title>cs285 DRL notes lecture 7: value function methods</title><link>http://localhost:1313/post/cs285_lecture7/lecture7/</link><pubDate>Wed, 16 Sep 2020 09:43:42 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture7/lecture7/</guid><description>&lt;p>本章学习基于值函数的强化学习方法。&lt;/p></description></item><item><title>Docker安装和使用</title><link>http://localhost:1313/post/docker%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 14 Sep 2020 10:15:36 +0800</pubDate><guid>http://localhost:1313/post/docker%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</guid><description>&lt;p>本文介绍了docker、nvidia-docker安装以及多阶段打包镜像的过程。&lt;/p></description></item><item><title>Python调试</title><link>http://localhost:1313/post/python_debug/</link><pubDate>Thu, 10 Sep 2020 11:11:14 +0800</pubDate><guid>http://localhost:1313/post/python_debug/</guid><description>&lt;p>本文介绍了使用PDB、GDB等调试方法，并使用cProfile进行性能分析。&lt;/p></description></item><item><title>cs285 DRL notes lecture 6: Actor-Critic methods</title><link>http://localhost:1313/post/cs285_lecture6/actor-critic-methods/</link><pubDate>Tue, 08 Sep 2020 17:18:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture6/actor-critic-methods/</guid><description>回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&amp;lsquo;reward-to-go&amp;rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{</description></item><item><title>PyTorch Cookbook</title><link>http://localhost:1313/post/pytorch_cookbook/</link><pubDate>Tue, 01 Sep 2020 17:09:18 +0800</pubDate><guid>http://localhost:1313/post/pytorch_cookbook/</guid><description/></item><item><title>cs285 DRL notes chapter 3: policy gradient methods</title><link>http://localhost:1313/post/cs285_lecture5/pg/</link><pubDate>Sat, 29 Aug 2020 16:00:06 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture5/pg/</guid><description>回顾强化学习的目标，我们希望获得策略的最优参数$\theta^*$， $$ \theta^*=\underset{\theta}{argmax}\mathbb{E}_{\tau\sim p_{\theta}(\tau)}[\sum_{t=1}^{t=T}r(s_t, a_t)] $$ 这实际上是一个优化问题，因此我们可以使用多种优化方法来优化这个</description></item><item><title>Pybind11 Cmake tutorial</title><link>http://localhost:1313/post/pybind11_cmake_tutorial/</link><pubDate>Tue, 25 Aug 2020 14:32:08 +0800</pubDate><guid>http://localhost:1313/post/pybind11_cmake_tutorial/</guid><description/></item><item><title>cs285 DRL notes lecture 2: imitation learning</title><link>http://localhost:1313/post/cs285_lecture2/lecture2/</link><pubDate>Sun, 23 Aug 2020 15:13:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture2/lecture2/</guid><description>模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据</description></item><item><title>Git基本使用</title><link>http://localhost:1313/post/git%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link><pubDate>Tue, 11 Aug 2020 11:00:12 +0800</pubDate><guid>http://localhost:1313/post/git%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid><description>&lt;p>Git是分布式版本控制系统
集中式VS分布式：
1. 集中式版本控制系统，版本库集中存放在中央服务器，必须要联网才能工作,没有历史版本库。
2. 分布式版本控制系统，版本控制系统没有“中央服务器”，每个人电脑上都是一个完整的版本库。
3. 分布式系统优势：安全性更高，不需要联网，如果中央服务器故障，任何其他一个开发人员的本地都有最新的带有历史记录的版本库。&lt;/p></description></item><item><title>cs285 DRL notes lecture 4: RL introduction</title><link>http://localhost:1313/post/cs285_lecture4/lecture3/</link><pubDate>Mon, 03 Aug 2020 10:58:41 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture4/lecture3/</guid><description>强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要</description></item><item><title>LMDB数据库使用</title><link>http://localhost:1313/post/lmdb/</link><pubDate>Wed, 11 Mar 2020 14:29:51 +0800</pubDate><guid>http://localhost:1313/post/lmdb/</guid><description/></item><item><title>Python虚拟环境</title><link>http://localhost:1313/post/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</link><pubDate>Wed, 22 Jan 2020 14:44:20 +0800</pubDate><guid>http://localhost:1313/post/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</guid><description/></item><item><title>Python多线程</title><link>http://localhost:1313/post/python-duo-xian-cheng/</link><pubDate>Sun, 24 Nov 2019 17:04:39 +0000</pubDate><guid>http://localhost:1313/post/python-duo-xian-cheng/</guid><description>由于GIL的存在，Python的多线程实际上是伪多线程，在同一时刻只有一个线程。因此Python的多线程适用于IO密集型任务（文件读写，网络</description></item></channel></rss>