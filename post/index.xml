<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on RuChen Xu's Blog</title><link>http://localhost:1313/post/</link><description>Recent content in Posts on RuChen Xu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 29 Sep 2020 09:34:56 +0800</lastBuildDate><atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml"/><item><title>cs285 DRL notes chapter 8: Model-based Planning</title><link>http://localhost:1313/post/cs285_chapter8/chapter8/</link><pubDate>Tue, 29 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_chapter8/chapter8/</guid><description/></item><item><title>cs285 DRL notes chapter 7: Advanced Policy Gradients</title><link>http://localhost:1313/post/cs285_chapter7/chapter7/</link><pubDate>Wed, 23 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_chapter7/chapter7/</guid><description/></item><item><title>cs285 DRL notes chapter 6: Deep RL with Q-Functions</title><link>http://localhost:1313/post/cs285_chapter6/chapter6/</link><pubDate>Fri, 18 Sep 2020 15:46:38 +0800</pubDate><guid>http://localhost:1313/post/cs285_chapter6/chapter6/</guid><description/></item><item><title>cs285 DRL notes chapter 5: value function methods</title><link>http://localhost:1313/post/cs285_chapter5/chapter5/</link><pubDate>Wed, 16 Sep 2020 09:43:42 +0800</pubDate><guid>http://localhost:1313/post/cs285_chapter5/chapter5/</guid><description/></item><item><title>cs285 DRL notes chapter 4: Actor-Critic methods</title><link>http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/</link><pubDate>Tue, 08 Sep 2020 17:18:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/</guid><description>回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&amp;lsquo;reward-to-go&amp;rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{</description></item><item><title>PyTorch Cookbook</title><link>http://localhost:1313/post/pytorch_cookbook/</link><pubDate>Tue, 01 Sep 2020 17:09:18 +0800</pubDate><guid>http://localhost:1313/post/pytorch_cookbook/</guid><description/></item><item><title>cs285 DRL notes chapter 3: policy gradient methods</title><link>http://localhost:1313/post/cs285_chapter3/cs285-drl-notes-chapter-3-policy-gradient-methods/</link><pubDate>Sat, 29 Aug 2020 16:00:06 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter3/cs285-drl-notes-chapter-3-policy-gradient-methods/</guid><description>回顾强化学习的目标，我们希望获得策略的最优参数$\theta^*$， $$ \theta^*=\underset{\theta}{argmax}\mathbb{E}_{\tau\sim p_{\theta}(\tau)}[\sum_{t=1}^{t=T}r(s_t, a_t)] $$ 这实际上是一个优化问题，因此我们可以使用多种优化方法来优化这个</description></item><item><title>Pybind11 Cmake tutorial</title><link>http://localhost:1313/post/pybind11_cmake_tutorial/</link><pubDate>Tue, 25 Aug 2020 14:32:08 +0800</pubDate><guid>http://localhost:1313/post/pybind11_cmake_tutorial/</guid><description/></item><item><title>cs285 DRL notes chapter 2: imitation learning</title><link>http://localhost:1313/post/cs285_chapter2/cs285-drl-notes-chapter-2-imitation-learning/</link><pubDate>Sun, 23 Aug 2020 15:13:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter2/cs285-drl-notes-chapter-2-imitation-learning/</guid><description>模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据</description></item><item><title>cs285 DRL notes chapter 1: introduction</title><link>http://localhost:1313/post/cs285_chapter1/cs285-drl-notes-chapter-1-introduction/</link><pubDate>Mon, 03 Aug 2020 10:58:41 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter1/cs285-drl-notes-chapter-1-introduction/</guid><description>强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要</description></item><item><title>python多线程</title><link>http://localhost:1313/post/python-duo-xian-cheng/</link><pubDate>Sun, 24 Nov 2019 17:04:39 +0000</pubDate><guid>http://localhost:1313/post/python-duo-xian-cheng/</guid><description>由于GIL的存在，Python的多线程实际上是伪多线程，在同一时刻只有一个线程。因此Python的多线程适用于IO密集型任务（文件读写，网络</description></item><item><title>Distributed RL</title><link>http://localhost:1313/post/cs285_chapter17/lecture17/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter17/lecture17/</guid><description>{% include start-row.html %}
The main bottleneck of creating distributed RL algorithms is that we need to create our own datasets with improved policies. Unlike SL where the datasets are given. This means we need to create algorithmic changes alongside system changes when designing new parallel architectures.
History of large scale distributed RL 2013. Original DQN DQN parallelisation was not DeepMinds main focus when first presented in 2013. Nevertheless, understanding its implementation helps to get an idea of how the others work.</description></item><item><title>Inverse Reinforcement Learning (IRL)</title><link>http://localhost:1313/post/cs285_chapter15/lecture15/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter15/lecture15/</guid><description>{% include start-row.html %}
Until now we had to manually design the reward function. IRL automatically learns the reward function from expert demonstrations. In this case, instead of hard-coding a reward we would provide demonstrations.
{% include annotation.html %} This is different from imitation learning (IL) since IL does not reason about the outcome of actions, it just tries to mimic the demonstrations. {% include end-row.html %} {% include start-row.html %}</description></item><item><title>Meta-Reinforcement Learning</title><link>http://localhost:1313/post/cs285_chapter18/lecture20/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter18/lecture20/</guid><description>{% include start-row.html %}
So far we&amp;rsquo;ve seen: Standard RL: Learn an optimal policy (optimal action given a state) for a single task. I.e. fit network parameters for a given MDP: {% include end-row.html %} {% include start-row.html %}
Forward Transfer: Train on one task, transfer to a new one.
Multi-task learning: Train on multiple tasks, transfer to a new one.
{% include annotation.</description></item><item><title>Model-based Policy Learning</title><link>http://localhost:1313/post/cs285_chapter12/lecture12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter12/lecture12/</guid><description>{% include start-row.html %}
In the previous lecture: Model-based RL, we where planning trajectories (stochastic open-loop), by maximizing the expected reward over a sequence of actions: \begin{equation} a_1,&amp;hellip;,a_T = \arg \max_{a_1,&amp;hellip;,a_T} E \left[ \sum_t r(s_t, a_t) \mid a_1,&amp;hellip;, a_T \right] \end{equation}
Now we will build a policies capable of adapting to the situation (stochastic closed-loop), by maximizing a reward expectation:
\begin{equation} \pi = \arg \max_{\pi} E_{\tau \sim p(\tau)} \left[ \sum_t r(s_t, a_t) \right] \end{equation}</description></item><item><title>Model-based RL</title><link>http://localhost:1313/post/cs285_chapter9/lecture11/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter9/lecture11/</guid><description>{% include start-row.html %}
Idea: If we learn $$f(s_t, a_t) = s_{t+1}$$ (or $$p(s_{t+1} \mid s_t, a_t)$$ in stochastic envs.) we can apply last lecture techniques to get a policy.
Basic approaches System identification in classical robotics: {% include figure.html url=&amp;quot;/_rl/lecture_11/si_1.png&amp;quot;%}
Does it work? Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters.</description></item><item><title>Transfer and multi-task RL</title><link>http://localhost:1313/post/cs285_chapter16/lecture16/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter16/lecture16/</guid><description>{% include start-row.html %}
NOTE: Transfer Learning (TL) &amp;amp; Multi-task Learning (MTL) are still an open problem in DRL, this lecture is more a survey on latest research rather than some well-established methods.
TL Terminology:
Task: In the RL context, task is the same as MDP. Source domain: Initial task where the agent has been trained. Target domain: Task which we want to solve. (We aim to transfer knowledge from source to target domain).</description></item></channel></rss>