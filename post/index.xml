<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on RuChen Xu's Blog</title><link>http://localhost:1313/post/</link><description>Recent content in Posts on RuChen Xu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 29 Oct 2020 09:34:56 +0800</lastBuildDate><atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml"/><item><title>cs285 DRL notes lecture 22: Meta-Reinforcement Learning</title><link>http://localhost:1313/post/cs285_lecture22/meta_rl/</link><pubDate>Thu, 29 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture22/meta_rl/</guid><description/></item><item><title>cs285 DRL notes lecture 21: Transfer and multi-task RL</title><link>http://localhost:1313/post/cs285_lecture21/lecture21/</link><pubDate>Sat, 24 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture21/lecture21/</guid><description/></item><item><title>cs285 DRL notes lecture 20: Inverse Reinforcement Learning (IRL)</title><link>http://localhost:1313/post/cs285_lecture20/lecture20/</link><pubDate>Tue, 20 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture20/lecture20/</guid><description/></item><item><title>cs285 DRL notes lecture 12: Model-based Policy Learning</title><link>http://localhost:1313/post/cs285_lecture12/lecture12/</link><pubDate>Thu, 15 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture12/lecture12/</guid><description/></item><item><title>cs285 DRL notes lecture 11: Model-based RL</title><link>http://localhost:1313/post/cs285_lecture11/lecture11/</link><pubDate>Fri, 02 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture11/lecture11/</guid><description>Idea: If we learn $f(s_t, a_t) = s_{t+1}$ (or $p(s_{t+1} \mid s_t, a_t)$ in stochastic envs.) we can apply last lecture techniques to get a policy.
Basic approaches System identification in classical robotics: Does it work? Yes, if we design a good base policy and we hand hand-engineer a dynamics representation of the environment (using physics knowledge) and we just fit few parameters.
Distribution mismatch problem: When we design a policy, we might be extrapolating beyond the data distribution used to learn the physics model, making results to be far off.</description></item><item><title>cs285 DRL notes lecture 10: Model-based Planning</title><link>http://localhost:1313/post/cs285_lecture10/lecture10/</link><pubDate>Tue, 29 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture10/lecture10/</guid><description/></item><item><title>cs285 DRL notes lecture 9: Advanced Policy Gradients</title><link>http://localhost:1313/post/cs285_lecture9/lecture9/</link><pubDate>Wed, 23 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture9/lecture9/</guid><description/></item><item><title>cs285 DRL notes lecture 8: Deep RL with Q-Functions</title><link>http://localhost:1313/post/cs285_lecture8/lecture8/</link><pubDate>Fri, 18 Sep 2020 15:46:38 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture8/lecture8/</guid><description/></item><item><title>cs285 DRL notes lecture 7: value function methods</title><link>http://localhost:1313/post/cs285_lecture7/lecture7/</link><pubDate>Wed, 16 Sep 2020 09:43:42 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture7/lecture7/</guid><description/></item><item><title>cs285 DRL notes lecture 6: Actor-Critic methods</title><link>http://localhost:1313/post/cs285_lecture6/actor-critic-methods/</link><pubDate>Tue, 08 Sep 2020 17:18:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture6/actor-critic-methods/</guid><description>回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&amp;lsquo;reward-to-go&amp;rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{</description></item><item><title>PyTorch Cookbook</title><link>http://localhost:1313/post/pytorch_cookbook/</link><pubDate>Tue, 01 Sep 2020 17:09:18 +0800</pubDate><guid>http://localhost:1313/post/pytorch_cookbook/</guid><description/></item><item><title>cs285 DRL notes chapter 3: policy gradient methods</title><link>http://localhost:1313/post/cs285_lecture5/pg/</link><pubDate>Sat, 29 Aug 2020 16:00:06 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture5/pg/</guid><description>回顾强化学习的目标，我们希望获得策略的最优参数$\theta^*$， $$ \theta^*=\underset{\theta}{argmax}\mathbb{E}_{\tau\sim p_{\theta}(\tau)}[\sum_{t=1}^{t=T}r(s_t, a_t)] $$ 这实际上是一个优化问题，因此我们可以使用多种优化方法来优化这个</description></item><item><title>Pybind11 Cmake tutorial</title><link>http://localhost:1313/post/pybind11_cmake_tutorial/</link><pubDate>Tue, 25 Aug 2020 14:32:08 +0800</pubDate><guid>http://localhost:1313/post/pybind11_cmake_tutorial/</guid><description/></item><item><title>cs285 DRL notes lecture 2: imitation learning</title><link>http://localhost:1313/post/cs285_lecture2/lecture2/</link><pubDate>Sun, 23 Aug 2020 15:13:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture2/lecture2/</guid><description>模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据</description></item><item><title>cs285 DRL notes lecture 4: RL introduction</title><link>http://localhost:1313/post/cs285_lecture4/lecture3/</link><pubDate>Mon, 03 Aug 2020 10:58:41 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture4/lecture3/</guid><description>强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要</description></item><item><title>python多线程</title><link>http://localhost:1313/post/python-duo-xian-cheng/</link><pubDate>Sun, 24 Nov 2019 17:04:39 +0000</pubDate><guid>http://localhost:1313/post/python-duo-xian-cheng/</guid><description>由于GIL的存在，Python的多线程实际上是伪多线程，在同一时刻只有一个线程。因此Python的多线程适用于IO密集型任务（文件读写，网络</description></item><item><title>Distributed RL</title><link>http://localhost:1313/post/distributed_rl/distributed_rl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/distributed_rl/distributed_rl/</guid><description>The main bottleneck of creating distributed RL algorithms is that we need to create our own datasets with improved policies. Unlike SL where the datasets are given. This means we need to create algorithmic changes alongside system changes when designing new parallel architectures.
History of large scale distributed RL 2013. Original DQN DQN parallelisation was not DeepMinds main focus when first presented in 2013. Nevertheless, understanding its implementation helps to get an idea of how the others work.</description></item></channel></rss>