<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on RuChen Xu's Blog</title><link>http://localhost:1313/post/</link><description>Recent content in Posts on RuChen Xu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 16 Sep 2020 09:43:42 +0800</lastBuildDate><atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml"/><item><title>cs285 DRL notes chapter 5: value function methods</title><link>http://localhost:1313/post/cs285_chapter5/chapter5/</link><pubDate>Wed, 16 Sep 2020 09:43:42 +0800</pubDate><guid>http://localhost:1313/post/cs285_chapter5/chapter5/</guid><description/></item><item><title>cs285 DRL notes chapter 4: Actor-Critic methods</title><link>http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/</link><pubDate>Tue, 08 Sep 2020 17:18:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/</guid><description>回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&amp;lsquo;reward-to-go&amp;rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{</description></item><item><title>PyTorch Cookbook</title><link>http://localhost:1313/post/pytorch_cookbook/</link><pubDate>Tue, 01 Sep 2020 17:09:18 +0800</pubDate><guid>http://localhost:1313/post/pytorch_cookbook/</guid><description/></item><item><title>cs285 DRL notes chapter 3: policy gradient methods</title><link>http://localhost:1313/post/cs285_chapter3/cs285-drl-notes-chapter-3-policy-gradient-methods/</link><pubDate>Sat, 29 Aug 2020 16:00:06 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter3/cs285-drl-notes-chapter-3-policy-gradient-methods/</guid><description>回顾强化学习的目标，我们希望获得策略的最优参数$\theta^*$， $$ \theta^*=\underset{\theta}{argmax}\mathbb{E}_{\tau\sim p_{\theta}(\tau)}[\sum_{t=1}^{t=T}r(s_t, a_t)] $$ 这实际上是一个优化问题，因此我们可以使用多种优化方法来优化这个</description></item><item><title>cs285 DRL notes chapter 2: imitation learning</title><link>http://localhost:1313/post/cs285_chapter2/cs285-drl-notes-chapter-2-imitation-learning/</link><pubDate>Sun, 23 Aug 2020 15:13:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter2/cs285-drl-notes-chapter-2-imitation-learning/</guid><description>模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据</description></item><item><title>cs285 DRL notes chapter 1: introduction</title><link>http://localhost:1313/post/cs285_chapter1/cs285-drl-notes-chapter-1-introduction/</link><pubDate>Mon, 03 Aug 2020 10:58:41 +0000</pubDate><guid>http://localhost:1313/post/cs285_chapter1/cs285-drl-notes-chapter-1-introduction/</guid><description>强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要</description></item><item><title>python多线程</title><link>http://localhost:1313/post/python-duo-xian-cheng/</link><pubDate>Sun, 24 Nov 2019 17:04:39 +0000</pubDate><guid>http://localhost:1313/post/python-duo-xian-cheng/</guid><description>由于GIL的存在，Python的多线程实际上是伪多线程，在同一时刻只有一个线程。因此Python的多线程适用于IO密集型任务（文件读写，网络</description></item></channel></rss>