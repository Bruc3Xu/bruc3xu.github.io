<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes chapter 5: value function methods - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_chapter5/chapter5/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes chapter 5: value function methods">
<meta property="og:description" content>
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_chapter5/chapter5/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-09-16T09:43:42+08:00">
<meta property="article:modified_time" content="2020-09-16T09:43:42+08:00">
<meta itemprop=name content="cs285 DRL notes chapter 5: value function methods">
<meta itemprop=description content><meta itemprop=datePublished content="2020-09-16T09:43:42+08:00">
<meta itemprop=dateModified content="2020-09-16T09:43:42+08:00">
<meta itemprop=wordCount content="1242">
<meta itemprop=keywords content="reinforcement learning,cs285,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes chapter 5: value function methods">
<meta name=twitter:description content><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes chapter 5: value function methods</h1>
<div class=post-meta>
<span class=post-time> 2020-09-16 </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#隐式策略>隐式策略</a></li>
<li><a href=#策略更新>策略更新</a>
<ul>
<li><a href=#动态规划>动态规划</a></li>
</ul>
</li>
<li><a href=#fitted-value-iteration>fitted value iteration</a></li>
<li><a href=#fitted-q-iteration>fitted q iteration</a>
<ul>
<li><a href=#online-q-iteration>online q iteration</a></li>
<li><a href=#exploration-with-q-learning>exploration with Q-learning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<h2 id=隐式策略>隐式策略</h2>
<p>尽管值函数方法忽略策略梯度，我们仍需要获得一个策略。</p>
<p>$$
\underset{a_t}{argmax}A^\pi(s_t,a_t)
$$</p>
<p>表示在状态$s_t$和策略$\pi$下，$a_t$是最优的执行动作。至少优于任意$a\sim \pi(s_t,a_t)$，不管$\pi(s_t,a_t)$具体是什么。
那么，我们可以得到改进的策略为</p>
<p>$$
\pi^{'}(s_t,a_t)=
\begin{cases}
1,\ & \text{if}\ a_t=\underset{a_t}{argmax}A^\pi(s_t,a_t)\\<br>
0,\ & \text{otherwise}
\end{cases}
$$</p>
<p>策略$\pi^{'}$至少与策略$\pi$一样好，大多数情况下优于策略$\pi$。</p>
<h2 id=策略更新>策略更新</h2>
<p>策略$\pi^{'}$优于$\pi$，我们可以不断更新策略，如图。
<img src=/post/cs285_chapter5/policy_iter.png alt></p>
<p><strong>policy iteration</strong></p>
<hr>
<ol>
<li>估计优势函数$A^\pi(s_t,a_t)$ #policy evaulation</li>
<li>$\pi\leftarrow\pi^{'}$ //policy improvement</li>
</ol>
<hr>
<p>已知</p>
<p>$$
A^\pi(s_t,a_t)=r(s,a)+\gamma\mathbb{E}[V^\pi(s')]-V^\pi(s)
$$</p>
<p>因此，我们可以通过估计值函数$V^\pi(s)$来更新策略。</p>
<h3 id=动态规划>动态规划</h3>
<p>如果我们已知状态转移概率$p(s'|s,a)$并且状态$s$和动作$a$都是离散的，那么可以使用动态规划算法。</p>
<p>$$
V^\pi(s)\leftarrow \mathbb{E}_{a\sim\pi(a|s)} \left[r(s,a) + \gamma\mathbb{E}_{s'\sim p(s'|s,a)}\left[V^\pi(s')\right]\right]
$$</p>
<p>因为策略是确定的，则期望值为常数，上式简化为</p>
<p>$$
V^\pi(s)\leftarrow r(s,\pi(s)) + \gamma\mathbb{E}_{s'\sim p(s'|s,a)}\left[V^\pi(s')\right]
$$</p>
<p><strong>policy iteration</strong></p>
<hr>
<ol>
<li>估计值函数$V^\pi(s)\leftarrow r(s,\pi(s))+\gamma\mathbb{E}_{s'\sim p(s'|s,a)}\left[V^\pi(s')\right]$</li>
<li>$\pi\leftarrow\pi^{'}$</li>
</ol>
<hr>
<p>策略迭代是首先创建/更新值函数表格，然后策略更新，直到策略收敛。</p>
<p>同时</p>
<p>$$
\arg max _{a_t}A^\pi(s,a) = \arg max _{a_t}Q^\pi(s,a)
$$</p>
<p>因为二者只相差$V^\pi$，而这个值是与动作无关的。
$$Q^\pi(s,a) = r(s,a) + \gamma\mathbb{E}\left[V^\pi(s')\right]$$</p>
<p>因此，我们可以更新Q函数来得到最优策略。</p>
<p><strong>value iteration</strong></p>
<hr>
<ol>
<li>$Q(s,a)\leftarrow r(s,a)+\gamma\mathbb{E}[V(s')]$</li>
<li>$V(s)\leftarrow max_aQ(s,a)$</li>
</ol>
<hr>
<p>值迭代是首先创建Q函数表格，然后计算值函数（相当于策略更新），直到Q函数表格收敛。</p>
<p>最后得到最优策略
$$
\pi^*(s,a) = \arg max _{a}Q(s,a)
$$</p>
<h2 id=fitted-value-iteration>fitted value iteration</h2>
<p>动态规划算法使用表格来记录值函数，当动作空间和状态空间过大时，难以适用，我们使用神经网络来近似值函数。</p>
<p><strong>fitted value iteration</strong></p>
<hr>
<ol>
<li>$y_i\leftarrow max_{a_i}(r(s_i,a_i)+\gamma\mathbb{E}[V(s_i^{'})])$，仍然需要知道所有动作以及对应的奖励值</li>
<li>$\phi\leftarrow \arg min_\phi{1\over2}\sum_i||V_\phi(s_i)-y_i||^2$</li>
</ol>
<hr>
<h2 id=fitted-q-iteration>fitted q iteration</h2>
<p>因为大多数情况下，我们并不知道环境的模型，因此无法进行value iteration。
我们能否借鉴policy-iteration到value-iteration的过程来实现Q iteration？
参照fitted value iteration，首先是构建q值target。</p>
<p>$$y_i\leftarrow r(s_i,a_i)+\gamma\mathbb{E}[V(s_i^{'})]$$</p>
<p>这里关键是上式中的第二项，如何来做近似。</p>
<ul>
<li>当前策略下所有next state的值的期望：利用多次采样来近似</li>
<li>$V(s_i^{'})\leftarrow max_aQ(s_i^{'},a_i^{'})$</li>
</ul>
<p>这样就得到<strong>fitted Q iteration</strong>
<img src=/post/cs285_chapter5/fitted_q_iteration.png alt></p>
<p>优缺点：</p>
<ul>
<li>可以使用off-policy样本。</li>
<li>只有一个Q网络，方差低。</li>
<li>不保证收敛。</li>
</ul>
<p>Q iteration是off-policy的，因为学习用到的数据与策略$\pi$是无关的。</p>
<p>我们实际优化的目标值为$\epsilon$，定义为Bellman Error。</p>
<p>$$\epsilon = \frac{1}{2}\mathbb{E}_{(s,a)\sim\beta}\left[\left(Q_\phi(s,a) - \left[r(s,a)+\gamma\max_{a'}Q_\phi(s',a')\right]\right)^2\right]$$</p>
<p>当$\epsilon=0$，我们得到最优Q函数$Q^*$和最优策略$\pi^*$。</p>
<blockquote>
<p>However, rather ironically, we do not know what we are optimizing in the previous steps, and this is a potential problem of the ﬁtted Q-learning algorithm, and most convergence guarantees are lost when we do not have the tabular case.</p>
</blockquote>
<h3 id=online-q-iteration>online q iteration</h3>
<p>online q iteration即一般意义上的Q-Learning算法。</p>
<hr>
<ol>
<li>执行动作$a_i$，得到transition$(s_i,a_i,s'_i,r_i)$</li>
<li>$y_i = r(s_i,a_i) + \gamma \max_{a'}Q_\phi(s_i,a_i,s'_i,r_i)$</li>
<li>$\phi \leftarrow \phi-\alpha\frac{dQ_\phi}{d\phi}(s_i,a_i)(Q_\phi(s_i,a_i) - y_i)$</li>
</ol>
<hr>
<h3 id=exploration-with-q-learning>exploration with Q-learning</h3>
<p>如果只使用argmax策略，则缺少对环境的探索。</p>
<p>epsilon-greedy</p>
<p>$$
\pi(a_t|s_t)=
\begin{cases}
1-\epsilon\ \text{if}\ a_t=\arg max_{a_t}Q_\phi(s_t,a_t)\\<br>
\epsilon/({A}-1)\ \text{otherwise}
\end{cases}
$$</p>
<p>Boltzmann exploration</p>
<p>$$
\pi(a_t|s_t)\propto exp(Q_\phi(s_t,a_t))
$$</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
<a href=/tags/cs285/>cs285</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/cs285_chapter6/chapter6/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">cs285 DRL notes chapter 6: Deep RL with Q-Functions</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/>
<span class="next-text nav-default">cs285 DRL notes chapter 4: Actor-Critic methods</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>