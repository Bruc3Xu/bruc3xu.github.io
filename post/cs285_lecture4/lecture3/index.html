<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes lecture 4: RL introduction - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content="强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要"><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_lecture4/lecture3/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes lecture 4: RL introduction">
<meta property="og:description" content="强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_lecture4/lecture3/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-08-03T10:58:41+00:00">
<meta property="article:modified_time" content="2020-08-03T10:58:41+00:00">
<meta itemprop=name content="cs285 DRL notes lecture 4: RL introduction">
<meta itemprop=description content="强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要"><meta itemprop=datePublished content="2020-08-03T10:58:41+00:00">
<meta itemprop=dateModified content="2020-08-03T10:58:41+00:00">
<meta itemprop=wordCount content="1407">
<meta itemprop=keywords content="reinforcement learning,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes lecture 4: RL introduction">
<meta name=twitter:description content="强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes lecture 4: RL introduction</h1>
<div class=post-meta>
<span class=post-time> 2020-08-03 </span>
<div class=post-category>
<a href=/categories/cs285/> cs285 </a>
</div>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#强化学习中的术语介绍>强化学习中的术语介绍。</a></li>
<li><a href=#状态值函数和q函数>状态值函数和Q函数</a>
<ul>
<li><a href=#q函数>Q函数</a></li>
<li><a href=#状态值函数>状态值函数</a></li>
</ul>
</li>
<li><a href=#reinforcement-learning-anatomy>Reinforcement Learning Anatomy</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。</p>
<h2 id=强化学习中的术语介绍>强化学习中的术语介绍。</h2>
<p>强化学习的主要角色是智能体和环境，环境是智能体存在和互动的世界。智能体在每一步的交互中，如图所示，
<img src=/post/cs285_lecture4/rl_diagram.png alt="RL diagram">
都会获得对于所处环境状态的观察（可能只是部分环境状态），
然后决定下一步要执行的动作。环境的状态会在智能体执行动作后发生变化，但也可能会自行发生改变。
智能体还感知来自环境的奖励信号，奖励信号告诉了智能体当前环境状态的好坏。智能体的目标是最大化其累积奖励，称为回报(return)。</p>
<p>在$t$时刻, 定义系统的状态为$s_t$。 系统的状态可以是系统本身的属性。 定义智能体的动作为$a_t$。策略{policy}是一个从状态到动作的映射，是一种决策规则。</p>
<p>策略分为确定性策略$a_t=\pi_\theta(s_t)$或者随机性策略（策略输出动作分布） $\pi_\theta(a_t|s_t)$。很多情形下，状态不是完全可观测的{fully observable}，因此智能体智能观察到部分（partially observe）状态$o_t$。此时策略建立在$o_t$之上。</p>
<p>定义环境状态转移概率{transition function}为系统模型。状态转移函数一般是随机的，表现为一定分布特征，定义为$p(s_{t+1}|s_t, a_t)$。状态转移函数表示在时间$t$的动作和状态，转移到下一个状态的概率。一系列连续的动作和状态组成了一条轨迹{trajectory}=$\tau$。</p>
<p>定义奖励函数{reward function}=$r(s,a)$，表示在当前状态下执行动作获得的奖励。
奖励函数需要人工设计或者逆强化学习学习得到。</p>
<p>在大部分强化学习环境中，认为状态转换具有马尔可夫性质{Markovian}，即在$t+1$时刻的状态只依赖于$t$时刻的状态。
如图解释了马尔可夫链，箭头代表着因果关系。
<img src=/post/cs285_lecture4/markov_chain.png alt="Markov Chain"></p>
<p>强化学习的目标是最大化累计奖励值。定义轨迹概率分布为$p_\theta(\tau)$，根据贝叶斯定理：
$$ p(\tau)= p(s_1,a_1,&mldr;,s_T,a_T) = p(s_1)\prod_{t=1}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$$
其中，$T$是回合的长度。</p>
<p>根据上式，可以计算出轨迹累计奖励期望值为</p>
<p>$$\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_t r(s_t,a_t)\right]$$</p>
<p>为了最大化累计奖励期望值，需要优化$\theta$，直到得到$\theta$满足下式：</p>
<p>$$ \theta = \underset{\theta}{argmax}\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[\sum_t r(s_t,a_t)\right] $$</p>
<h2 id=状态值函数和q函数>状态值函数和Q函数</h2>
<p>为了简化形式，引入两个函数。状态动作价值函数即Q函数和状态价值函数V。在DRL中，这两个函数一般是需要使用神经网络建模的对象。</p>
<h3 id=q函数>Q函数</h3>
<p>Q函数记为$Q(s_t,a_t)$，衡量在状态$s_t$下采取动作$a_t$的价值。 具体计算：从当前时刻状态$s_t$和动作$a_t$开始的奖励值的期望。
$$Q^\pi(s_t,a_t) = \sum_{t'=t}^T{\mathbb{E}_{\pi_\theta}[r(s_t',a_t')|s_t,a_t]}$$</p>
<h3 id=状态值函数>状态值函数</h3>
<p>状态值函数衡量状态$s_t$的价值。定义为$V^\pi(s_t) =\sum_{t'=t}^T{\mathbb{E}_{\pi_\theta}[r(s_t',a_t')|s_t]}$。</p>
<p>同样根据贝叶斯定理，状态值函数与Q函数的关系为： $V^\pi(s_t)=\mathbb{E}_{a_t\sim\pi(a_t|s_t)}[Q^\pi(s_t,a_t)]$.</p>
<p>更近一步，如果将初始状态的所有可能状态值相加，就可以得到强化学习的目标值。
$\mathbb{E}_{s_1\sim p(s_1)}[V^\pi(s_1)]$，其中$p(s_1)$代表所有可能的初始状态。</p>
<h2 id=reinforcement-learning-anatomy>Reinforcement Learning Anatomy</h2>
<p>如图，强化学习过程主要分为3个步骤：1）生成数据：根据不同的算法可以使用各种策略与环境交互；2）估计回报或者拟合环境模型（model based）；3）改进当前策略；
<img src=/post/cs285_lecture4/rl_anatomy.png alt="RL anatomy">
强化学习的关键就在于正确高效地完成这三个步骤。</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2020-08-03
</span>
</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/cs285_lecture2/lecture2/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">cs285 DRL notes lecture 2: imitation learning</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/python-duo-xian-cheng/>
<span class="next-text nav-default">python多线程</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>