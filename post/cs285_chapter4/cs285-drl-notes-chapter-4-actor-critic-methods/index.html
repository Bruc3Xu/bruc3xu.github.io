<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes chapter 4: Actor-Critic methods - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&amp;lsquo;reward-to-go&amp;rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{"><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.0 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes chapter 4: Actor-Critic methods">
<meta property="og:description" content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&lsquo;reward-to-go&rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2021-08-24T17:18:48+00:00">
<meta property="article:modified_time" content="2021-08-24T17:18:48+00:00">
<meta itemprop=name content="cs285 DRL notes chapter 4: Actor-Critic methods">
<meta itemprop=description content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&lsquo;reward-to-go&rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{"><meta itemprop=datePublished content="2021-08-24T17:18:48+00:00">
<meta itemprop=dateModified content="2021-08-24T17:18:48+00:00">
<meta itemprop=wordCount content="2940">
<meta itemprop=keywords content="reinforcement learning,cs285,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes chapter 4: Actor-Critic methods">
<meta name=twitter:description content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&lsquo;reward-to-go&rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes chapter 4: Actor-Critic methods</h1>
<div class=post-meta>
<span class=post-time> 2021-08-24 </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#值函数作为基线>值函数作为基线</a></li>
<li><a href=#值函数拟合>值函数拟合</a></li>
<li><a href=#策略评估>策略评估</a></li>
<li><a href=#从策略评估到actor-critic>从策略评估到actor-critic</a>
<ul>
<li><a href=#折扣系数>折扣系数</a></li>
</ul>
</li>
<li><a href=#actor-critic算法结构设计>actor-critic算法结构设计</a></li>
<li><a href=#critic作为baseline>critic作为baseline</a></li>
<li><a href=#control-variate-action-dependent-baseline>control variate: action dependent baseline</a></li>
<li><a href=#eligibility-traces--n-step-returns>Eligibility traces & n-step returns</a></li>
<li><a href=#gae>GAE</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>回顾策略梯度算法，
$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$
我们使用'&lsquo;reward-to-go&rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{i,t}$的回报。上一章我们证明了这种近似具有很高的方差，这一章我们会使用其他方法来解决这个问题。</p>
<h2 id=值函数作为基线>值函数作为基线</h2>
<p>我们使用平均rewrad-to-go作为基线，定义为$\hat{Q}_{i,t}$表示在状态$s_{i,t}$采取动作$a_{i,t}$的期望奖励的估计，这一估计值并不准确。</p>
<p>而$Q(s_t, a_t) =\sum_{t'=t}^T{\mathbb{E}_{\pi_\theta}[r(s_t',a_t')|s_t]}$，代表reward-to-go的真实期望值，优于上式的近似表示。</p>
<p>$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta (a_{i,t}|s_{i,t})Q(s_{i,t},a_{i,t})$$</p>
<p>同样baseline为$b_t = \frac{1}{N}\sum_i Q(s_{i,t},a_{i,t})$</p>
<p>$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta (a_{i,t}|s_{i,t})(Q(s_{i,t},a_{i,t}) - b_t)$$</p>
<p>值函数$V^\pi(s_t)=\mathbb{E}_{a_t\sim \pi_\theta(a_t|s_t)}[Q^\pi(s_t,a_t)]$表示从状态$s_t$开始的期望总奖励。</p>
<p>优势函数$A^\pi(s_t,a_t)=Q^\pi(s_t,a_t)-V^\pi(s_t)$表示在状态$s_t$下采取动作$a_t$的好坏。</p>
<p>$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta (a_{i,t}|s_{i,t})A^\pi(s_{i,t},a_{i,t})$$</p>
<h2 id=值函数拟合>值函数拟合</h2>
<p>$Q^\pi,V^\pi,A^\pi$，我们需要拟合什么？
其中，</p>
<p>$$
\begin{aligned}
Q^\pi(s_t, a_t) &=r(s_t,a_t)+\sum_{t'=t+1}^T{\mathbb{E}_{\pi_\theta}[r(s_{t'},a_{t'})|s_t]}\\<br>
&=r(s_t,a_t)+\mathbb{E}_{s_{t+1}\sim p(s_{t+1}|s_t,a_t)}[V^\pi(s_{t+1})]\\<br>
&\simeq r(s_t,a_t)+V^\pi(s_{t+1})\\<br>
A^\pi(s_t,a_t)&\simeq r(s_t,a_t)+V^\pi(s_{t+1}) - V^\pi(s_t)
\end{aligned}
$$</p>
<p>因此，我们只需要拟合$V^\pi(s_t)$。</p>
<h2 id=策略评估>策略评估</h2>
<p>和策略梯度方法相似，我们可以使用蒙特卡洛方法来评估值函数$V^\pi(s_t)$。</p>
<p>$$
\begin{aligned}
V^\pi(s_t) &= \sum_{t'=t}^T\mathbb{E}_{\pi_\theta}[r(s_{t'},a_{t'})|s_t]\\<br>
&\simeq {1 \over N} \sum_{i=1}^N\sum_{t'=t}^Tr(s_{t'},a_{t'})|s_t\\<br>
&\simeq \sum_{t'=t}^Tr(s_{t'},a_{t'})|s_t
\end{aligned}
$$</p>
<p>我们使用监督学习来拟合值函数，损失函数</p>
<p>$$
L(\phi)=1/2\sum_i||\hat{V}_\phi^\pi(s_i)-y_i||^2
$$</p>
<p>此处的$y_i$即为$V^\pi(s_i)$，$\hat{V}_\phi^\pi$是具有参数$\phi$的神经网络。</p>
<p>更进一步，使用bootstrap（自举）方法可以得到$y_i$</p>
<div>
\begin{aligned}
y_{i,t} &= \sum_{t'=t}^{T}\mathbb{E}_{\pi_\theta}[r(s_{t'}, a_{t'})|s_{i,t}]\\
&\simeq r(s_{i,t}, a_{i,t})+\sum_{t'=t+1}^T[r(s_{t'}, a_{t'})|s_{i,t+1}]\\
&\simeq r(s_{i,t}, a_{i,t})+V^\pi(s_{i,t+1})\\
\end{aligned}
</div>
<p>尽管上式对状态价值的估计是有偏的，但具有较低的方差。
最终的MSE损失形式为</p>
<p>$$
L(\phi)=1/2\sum_i||\hat{V}_\phi^\pi(s_i)-(r(s_{i,t}, a_{i,t})+\hat{V}_\phi^\pi(s_{i+1}))||^2
$$</p>
<h2 id=从策略评估到actor-critic>从策略评估到actor-critic</h2>
<p>batch actor-critic算法</p>
<hr>
<ol>
<li>从策略$\pi_\theta(a|s)$采样${s_i, a_i}$</li>
<li>使用采样得到的累计奖励来拟合$\hat{V}_\phi^\pi(s)$</li>
<li>评估$\hat{A}^\pi(s_i,a_i)=r(s_i,a_i,s_{i+1})+\hat{V}_\phi^\pi(s_{i'})-\hat{V}_\phi^\pi(s_i)$</li>
<li>$\nabla_\theta J(\theta)\simeq \sum_i\nabla_\theta \log\pi_\theta(a_i|s_i)\hat{A}_\phi(s_i,a_i)$</li>
<li>$\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$</li>
</ol>
<hr>
<h3 id=折扣系数>折扣系数</h3>
<p>当回合长度变的无穷大时，$\hat{V}_\phi^\pi$在许多情况下也会变的无穷大。这时我们使用折扣奖励来避免这种情况，另一方面的考虑是我们认为短期奖励要优于长期奖励。此时</p>
<p>$$
y_{i,t}\simeq r(s_{i,t},a_{i,t})+\gamma\hat{V}_\phi^\pi(s_{i,t+1})
$$</p>
<p>对于策略梯度使用折扣系数，这里有两种方式
$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T \gamma^{t'-t}r(s_{i,t'},a_{i,t'})\right)$$
第二种：
$$\begin{aligned}
\nabla_\theta J(\theta) &\simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t=1}^T \gamma^{t-1}r(s_{i,t},a_{i,t})\right)\\<br>
&\simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T \gamma^{t'-1}r(s_{i,t'},a_{i,t'})\right) ;\mathrm{(causality)}\\<br>
&\simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\gamma^{t-1}\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T \gamma^{t'-t}r(s_{i,t'},a_{i,t'})\right)
\end{aligned}$$
两种都是rewrad-to-go形式，一个是$\gamma^0, \gamma^1, \gamma^2&mldr;\gamma^{t'-t}$，而另一种系数则是$\gamma^{t-1}(\gamma^0, \gamma^1, \gamma^2&mldr;\gamma^{t'-t})$。
直观上，第二种方式采样时间靠后的样本梯度权重更小。实际中，我们采用第一种方式，这种方式的方差更小<a href=#refer-anchor><sup>1</sup></a>。</p>
<p>使用折扣系数的batch actor-critic算法</p>
<hr>
<ol>
<li>从策略$\pi_\theta(a|s)$采样${s_i, a_i}$</li>
<li>使用采样得到的累计奖励来拟合$\hat{V}_\phi^\pi(s)$</li>
<li>评估$\hat{A}^\pi(s_i,a_i)=r(s_i,a_i,s_{i+1})+\gamma\hat{V}_\phi^\pi(s_{i'})-\hat{V}_\phi^\pi(s_i)$</li>
<li>$\nabla_\theta J(\theta)\simeq \sum_i\nabla_\theta \log\pi_\theta(a_i|s_i)\hat{A}_\phi(s_i,a_i)$</li>
<li>$\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$</li>
</ol>
<hr>
<p>在线的actor-critic算法</p>
<hr>
<ol>
<li>动作$a\sim\pi_\theta(a|s)$，得到transition采样${s, a,s',r}$</li>
<li>使用target ${r+\gamma\hat{V}_\phi^\pi(s')}$来拟合$\hat{V}_\phi^\pi(s)$</li>
<li>评估$\hat{A}^\pi(s,a)=r+\gamma\hat{V}_\phi^\pi(s')-\hat{V}_\phi^\pi(s)$</li>
<li>$\nabla_\theta J(\theta)\simeq \sum_i\nabla_\theta \log\pi_\theta(a|s)\hat{A}_\phi(s,a)$</li>
<li>$\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$</li>
</ol>
<hr>
<h2 id=actor-critic算法结构设计>actor-critic算法结构设计</h2>
<p><img src=/post/cs285_chapter4/two-network-design.png alt></p>
<p>使用两个神经网络分别来近似值函数和策略函数，优点：简单、有效，易于使用；缺点：在一些情况下，值函数和策略函数需要共享特征，特别在特征比较复杂时。</p>
<p><img src=/post/cs285_chapter4/shared_network_design.png alt></p>
<p>另一方面是使用并行的worker来批次更新效果会更好。值得注意的是，异步并行的actor-critic算法，行为策略会落后于学习策略，这回出现一些问题，具体在A3C算法中有描述。
<img src=/post/cs285_chapter4/batch-online-ac.png alt></p>
<h2 id=critic作为baseline>critic作为baseline</h2>
<p>actor-critic算法：有偏的，低方差</p>
<p>$$
\nabla_\theta J(\theta)\simeq {1 \over N}\sum_i^N\sum_t^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})(r(s_{i,t},a_{i,t},s_{i,t+1})+\gamma\hat{V}_\phi^\pi(s_{i,t+1})-\hat{V}_\phi^\pi(s_{i,t}))
$$</p>
<p>策略梯度算法：无偏，高方差</p>
<p>$$
\nabla_\theta J(\theta)\simeq {1 \over N}\sum_i^N\sum_t^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})((\sum_{t'=t}^T\gamma^{t'-t} r(s_{i,t'},a_{i,t'})) - b)
$$</p>
<p>state-dependent baseline：无偏，较低方差</p>
<p>$$
\nabla_\theta J(\theta)\simeq {1 \over N}\sum_i^N\sum_t^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})((\sum_{t'=t}^T\gamma^{t'-t} r(s_{i,t'},a_{i,t'})) - \hat{V}_\phi^\pi(s_{i,t}))
$$</p>
<h2 id=control-variate-action-dependent-baseline>control variate: action dependent baseline</h2>
<p>控制变量法（英语：control variates）是在蒙特卡洛方法中用于减少方差的一种技术方法。该方法通过对已知量的了解来减少对未知量估计的误差。</p>
<p>强化学习中，QProp算法采用了control variate的思想，在无偏的情况下来减少策略梯度中的方差。</p>
<h2 id=eligibility-traces--n-step-returns>Eligibility traces & n-step returns</h2>
<p>eligibility trace称为资格迹，最先在$TD(\lambda)$中使用。$e_t$表示第t步资格迹，是值函数的优化微分值。
其优化的技术称为(backward view)。仔细观察公式可以发现$e_t$的算法中包含了以前的微分值，考虑了过去的价值梯度对更新参数$\theta$的影响。</p>
<p>$$
\begin{aligned}
e_0&\doteq 0\\<br>
e_t&\doteq \nabla\hat{V}_{\theta}(s_t) + \lambda\gamma e_{t-1}\\<br>
\theta &\leftarrow \theta + \alpha (r_{t+1}+\gamma\hat{V}_{\theta}(s_{t+1})-\hat{V}_{\theta}(s_t))e_t <br>
\end{aligned}
$$</p>
<p>对比两种方法计算的策略梯度：</p>
<ul>
<li>Monte-Carlo方法：无偏，高方差。</li>
<li>actor-critic方法：有偏，低方差。</li>
</ul>
<p>我们希望结合这两种方法，使用Monte-Carlo方法来计算近期的值估计，使用actor-critic方法来计算将来的值估计。结合下图，我们希望在方差变的过大之前截断轨迹，只使用
n步数据。
<img src=/post/cs285_chapter4/traj_var.png alt=traj_Var></p>
<p>$$
\begin{aligned}
Q&=\mathbb{E}[r_0+\gamma r_1+&mldr;+\gamma^{n}r_n]\quad\text{Monte-Carlo} \\<br>
&=\mathbb{E}[r_0+\gamma V^\pi(s_1)]\quad \text{1 step TD} \\<br>
&=\mathbb{E}[r_0+\gamma r_1+\gamma^2 V^\pi(s_2)]\quad \text{2 step TD} \\<br>
&=\mathbb{E}[r_0+\gamma r_1+&mldr;+\gamma^{n}r_n+\gamma^n V^\pi(s_n)]\quad \text{n step TD} \\<br>
\end{aligned}
$$</p>
<p>n-step优势函数为：</p>
<div>
$$
\hat{A}_n^\pi(s_t,a_t)=[\sum_{t'=t}^{t+n}\gamma^{t'-t}r(s_{t'},a_{t'})]+\gamma^n\hat{V}_\phi^\pi(s_{t+n})-\hat{V}_\phi^\pi(s_t)
$$
</div>
<h2 id=gae>GAE</h2>
<p>GAE(Generalized Advantage Estimator，通用优势估计)。</p>
<p>上一小节我们介绍了n-step return，但我们不一定只能选择一个n，可以将所有n对应优势估计进行加权求和。</p>
<div>
$$
\hat{A}_{GAE}^\pi(s_t,a_t)=\sum_{n=1}^\infty w_n\hat{A}_{n}^\pi(s_t,a_t)
$$
</div>
<p>权重的选择是</p>
<div>
$$
\begin{aligned}
\sum_{i=1}^Nw_i&=1\\
w_n&\propto\lambda^{n-1}\\
\lambda+\lambda^1+\lambda^2+...+\lambda^n &= {1\over 1-\lambda}\\
\Rightarrow w_n &= \lambda^{n-1} (1 - \lambda)
\end{aligned}
$$
</div>
<p>由</p>
<p>$$
\begin{aligned}
\hat{A}_1^\pi(s_t,a_t)&=\delta_t=r(s_{t},a_{t})+\gamma^1\hat{V}_\phi^\pi(s_{t+1})-\hat{V}_\phi^\pi(s_t)\\<br>
\hat{A}_2^\pi(s_t,a_t)&=r(s_t,a_t)+\gamma r(s_{t+1},a_{t+1})+\gamma^2\hat{V}_\phi^\pi(s_{t+2})-\hat{V}_\phi^\pi(s_{t+1})\\<br>
&=\delta_t+\gamma\delta_{t+1}\\<br>
\hat{A}_n^\pi(s_t,a_t)&=\sum_{i=0}^{n-1}\gamma^i\delta_{t+i}\\<br>
\end{aligned}
$$</p>
<p>因此，</p>
<div>
$$
\begin{aligned}
\hat{A}_{GAE}^\pi&=(1-\lambda)(\hat{A}_{1}^\pi+\lambda\hat{A}_{2}^\pi...\lambda^{n-1}\hat{A}_{n}^\pi)\\
&=(1-\lambda)(\delta_t+\lambda(\delta_t+\gamma\delta_{t+1})+\lambda^2(\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2})...)\\
&=(1-\lambda)[\delta_t(1+\lambda+\lambda^2+...)+\gamma\delta_{t+1}(\lambda+\lambda^2+...)+\gamma^2\delta_{t+2}(\lambda^2+...)]\\
&=(1-\lambda)[{\delta_t\over1-\lambda}+{\gamma\delta_{t+1}\lambda\over1-\lambda}+{\gamma^2\delta_{t+2}\lambda^2\over1-\lambda}+...]\\
&=\sum_{i=0}^{\infty}\delta_{t+i}(\lambda\gamma)^i
\end{aligned}
$$
</div>
<p>当$\lambda=0$时，$\hat{A}_{GAE}^\pi=\delta_t$，即TD Error。</p>
<p>当$\lambda=1$时，$\hat{A}_{GAE}^\pi=\sum_{i=0}^{\infty}\delta_{t+i}\gamma^i=\sum_{i=0}^{\infty}r_{t+i}\gamma^i - V(s_t)$，即Monte-Carlo Advantage。</p>
<hr>
<p>初始化gae=0<br>
$\delta=r_t+\gamma V(s_{t+1})m_t-V(s_t)$<br>
更新gae $gae_t=\delta + \gamma\ast\lambda \ast m_t\ast gae_{t+1}$<br>
计算returns $R_t(s_t,a_t)=gae_t+V(s_t)$</p>
<hr>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>get_advantages</span><span class=p>(</span><span class=n>values</span><span class=p>,</span> <span class=n>masks</span><span class=p>,</span> <span class=n>rewards</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    masks: 表示回合是否结束
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=n>returns</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>gae</span> <span class=o>=</span> <span class=mi>0</span>
    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>rewards</span><span class=p>))):</span>
        <span class=n>delta</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>values</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>masks</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>values</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
        <span class=n>gae</span> <span class=o>=</span> <span class=n>delta</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>lmbda</span> <span class=o>*</span> <span class=n>masks</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*</span> <span class=n>gae</span>
        <span class=n>returns</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>gae</span> <span class=o>+</span> <span class=n>values</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
    <span class=n>returns</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>reversed</span><span class=p>(</span><span class=n>returns</span><span class=p>))</span>
    <span class=n>adv</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>returns</span><span class=p>)</span> <span class=o>-</span> <span class=n>values</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
    <span class=k>return</span> <span class=n>returns</span><span class=p>,</span> <span class=p>(</span><span class=n>adv</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>adv</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>adv</span><span class=p>)</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><div id=refer-anchor></div>
[1] P. Thomas, “Bias in natural actor-critic algorithms,” in International conference onmachine learning, 2014, pp. 441–448.
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2021-08-24
</span>
</p>
</div>
<div class=post-reward>
<input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>Reward</label>
<div class=qr-code>
<label class=qr-code-image for=reward>
<img class=image src=/img/reward/wechat.png>
<span>wechat</span>
</label>
<label class=qr-code-image for=reward>
<img class=image src=/img/reward/alipay.png>
<span>alipay</span>
</label>
</div>
</div><footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
<a href=/tags/cs285/>cs285</a>
</div>
<nav class=post-nav>
<a class=next href=/post/pytorch_cookbook/>
<span class="next-text nav-default">PyTorch Cookbook</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>