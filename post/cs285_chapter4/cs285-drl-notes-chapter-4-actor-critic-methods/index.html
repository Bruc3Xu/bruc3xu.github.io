<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes chapter 4: Actor-Critic methods - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用``reward-to-go'' 来近似在状态$s_{i,t}$采取动作 $a_{i,t}$的回报。上一章"><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.87.0 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes chapter 4: Actor-Critic methods">
<meta property="og:description" content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用``reward-to-go'' 来近似在状态$s_{i,t}$采取动作 $a_{i,t}$的回报。上一章">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_chapter4/cs285-drl-notes-chapter-4-actor-critic-methods/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2021-08-24T17:18:48+00:00">
<meta property="article:modified_time" content="2021-08-24T17:18:48+00:00">
<meta itemprop=name content="cs285 DRL notes chapter 4: Actor-Critic methods">
<meta itemprop=description content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用``reward-to-go'' 来近似在状态$s_{i,t}$采取动作 $a_{i,t}$的回报。上一章"><meta itemprop=datePublished content="2021-08-24T17:18:48+00:00">
<meta itemprop=dateModified content="2021-08-24T17:18:48+00:00">
<meta itemprop=wordCount content="431">
<meta itemprop=keywords content><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes chapter 4: Actor-Critic methods">
<meta name=twitter:description content="回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用``reward-to-go'' 来近似在状态$s_{i,t}$采取动作 $a_{i,t}$的回报。上一章"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes chapter 4: Actor-Critic methods</h1>
<div class=post-meta>
<span class=post-time> 2021-08-24 </span>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#值函数作为基线>值函数作为基线</a></li>
<li><a href=#值函数拟合>值函数拟合</a></li>
<li><a href=#策略评估>策略评估</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>回顾策略梯度算法，
$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$
我们使用``reward-to-go'' 来近似在状态$s_{i,t}$采取动作 $a_{i,t}$的回报。上一章我们证明了这种近似具有很高的方差，这一章我们会使用其他方法来解决这个问题。</p>
<h2 id=值函数作为基线>值函数作为基线</h2>
<p>上一章我们为了减少方差，使用平均轨迹回报最为基线，如果我们使用平均rewrad-to-go作为基线，
$$
\bar{R_t'}=1/N\sum_{i=1}^{N}(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})
$$
而状态值函数衡量状态$s_t$的价值。定义为$V^\pi(s_t) =\sum_{t'=t}^T{\mathbb{E}_{\pi_\theta}[r(s_t',a_t')|s_t]}$，代表当前状态的期望价值，优于上式的近似表示。</p>
<p>$$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_{\theta}\log \pi_\theta (a_{i,t}|s_{i,t})\left(Q(s_{i,t},a_{i,t}) - V(s_{i,t})\right)$$
and the value function we used is a better approximation of the baseline $b_t = \frac{1}{N}\sum_i Q(s_{i,t},a_{i,t})$.</p>
<h2 id=值函数拟合>值函数拟合</h2>
<p>我们使用监督学习来拟合值函数，损失函数
$$
L(\phi)=1/2\sum_i||\hat{V}<em>\phi^\pi(s_i)-y_i||^2
$$
$y_i$是当前状态的期望价值，使用bootstrap（自举）方法可以得到
$$
\begin{aligned}
y</em>{i,t} &= \sum_{t'=t}^{T}\mathbb{E}_{\pi_\theta}[r(s_{t'}, a_{t'})|s_{i,t}]\<br>
&\simeq r(s_{i,t}, a_{i,t})+\sum_{t'=t+1}^T[r(s_{t'}, a_{t'})|s_{i,t+1}]\<br>
&\simeq r(s_{i,t}, a_{i,t})+V^\pi(s_{i,t+1})\<br>
\end{aligned}
$$
尽管上式对状态价值的估计是有偏的，但具有较低的方差。
最终的MSE损失形式为
$$
L(\phi)=1/2\sum_i||\hat{V}_\phi^\pi(s_i)-(r(s_{i,t}, a_{i,t})+\hat{V}_\phi^\pi(s_{i+1}))||^2
$$</p>
<h2 id=策略评估>策略评估</h2>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2021-08-24
</span>
</p>
</div>
<footer class=post-footer>
<nav class=post-nav>
<a class=next href=/post/cs285_chapter3/cs285-drl-notes-chapter-3-policy-gradient-methods/>
<span class="next-text nav-default">cs285 DRL notes chapter 3: policy gradient methods</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>