<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes lecture 9: Advanced Policy Gradients - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_lecture9/lecture9/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes lecture 9: Advanced Policy Gradients">
<meta property="og:description" content>
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_lecture9/lecture9/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-09-23T09:34:56+08:00">
<meta property="article:modified_time" content="2020-09-23T09:34:56+08:00">
<meta itemprop=name content="cs285 DRL notes lecture 9: Advanced Policy Gradients">
<meta itemprop=description content><meta itemprop=datePublished content="2020-09-23T09:34:56+08:00">
<meta itemprop=dateModified content="2020-09-23T09:34:56+08:00">
<meta itemprop=wordCount content="2781">
<meta itemprop=keywords content="reinforcement learning,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes lecture 9: Advanced Policy Gradients">
<meta name=twitter:description content><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes lecture 9: Advanced Policy Gradients</h1>
<div class=post-meta>
<span class=post-time> 2020-09-23 </span>
<div class=post-category>
<a href=/categories/cs285/> cs285 </a>
</div>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#policy-gradient-as-policy-iteration>Policy Gradient as Policy Iteration</a>
<ul>
<li><a href=#a-better-measure-of-divergence>A better measure of divergence</a></li>
</ul>
</li>
<li><a href=#enforcing-the-kl-constraint>Enforcing the KL constraint</a>
<ul>
<li><a href=#dual-gradient-descent>Dual Gradient Descent</a></li>
<li><a href=#natural-gradients>Natural Gradients</a></li>
<li><a href=#trust-region-policy-optimization>Trust Region Policy Optimization</a></li>
<li><a href=#proximal-policy-optimization>Proximal Policy Optimization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>本章会深入策略梯度算法，进一步学习
<strong>Natural Policy Gradient</strong>, <strong>Trust Region Policy Optimization</strong>, or <strong>Proximal Policy
Optimization</strong>等算法。</p>
<h2 id=policy-gradient-as-policy-iteration>Policy Gradient as Policy Iteration</h2>
<p>回顾Policy Gradient和Actor-Critic算法，
$$
\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T
\nabla_{\theta}\log \pi_{\theta}(a_{i, t} \vert s_{i, t}) A^{\pi}_{i, t}
$$</p>
<p>可以看作一下过程</p>
<ol>
<li>使用当前策略$\pi$估计$A^{\pi}_{i, t}$</li>
<li>基于$A^{\pi}(s_t, a_t)$来获得改进后的策略$\pi'$</li>
</ol>
<p>与Policy Iteration过程相同。 什么情况下Policy Gradient可以看作Policy Iteration？首先我们分析Policy Gradient中的<strong>policy improvement</strong>。</p>
<p>策略梯度：</p>
<p>$$
J(\theta) = E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t}\gamma^t r(s_t, a_t) \right]
$$</p>
<p>新策略参数$\theta'$和旧策略参数$\theta$ 的policy improvement：
$$
J(\theta') - J(\theta) = E_{\tau \sim p_{\theta'}(\tau)} \left[
\sum_t \gamma^t A^{\pi_{\theta}}(s_t, a_t) \right]
$$
旧策略$\theta$的advantage关于新策略$\theta'$的trajectory的期望值。</p>
<p>如果按照policy iteration的流程，在improvement中，也就只需要使得每步提升最大，找到新的parameter使得等式的右方最大化即可。但是improvement是计算新策略$\theta'$,
的轨迹期望，而我们当前的样本都是基于旧策略$\theta$采集的。</p>
<p>把improvement展开：</p>
<p>$$
E_{\tau \sim p_{\theta'}(\tau)} \left[ \sum_t \gamma^t A^{\pi_{\theta}}(s_t, a_t) \right] =
\sum_t E_{s_{t} \sim p_{\theta'}(s_{t})} \left[ E_{a_{t} \sim \pi_{\theta'}(a_{t})} \left[
\gamma^t A^{\pi_{\theta}}(s_t, a_t)
\right]\right]
$$</p>
<p>然后使用<a href=https://en.wikipedia.org/wiki/Importance_sampling>Importance Sampling</a>将动作从分布$\pi_{\theta'}(a_t)$转换到分布$\pi_{\theta}(a_t)$:
$$
E_{\tau \sim p_{\theta'}(\tau)} \left[ \sum_t \gamma^t A^{\pi_{\theta}}(s_t, a_t) \right] =
\sum_t E_{s_{t} \sim p_{\theta'}(s_{t})} \left[ E_{a_{t} \sim \pi_{\theta}(a_{t})} \left[
\frac{\pi_{\theta^{\prime}}(a_t)}{\pi_{\theta}(a_t)} \gamma^t A^{\pi_{\theta}}(s_t, a_t)
\right]\right]
$$</p>
<p>但是现在状态$s_t$仍然是来自于分布$p_{\theta'}$。
如果$\pi$和$\pi'$的**total variation divergence**（总变异散度，两个分布每个元素差值绝对值之和）小于$\epsilon$，</p>
<p>$$
\vert \pi_{\theta'}(a_t \vert s_t) - \pi_{\theta}(a_t \vert s_t) \vert \le \epsilon
$$</p>
<p>那么$p_{\theta'}$和$p_{\theta}$的总变异散度有上界：</p>
<p>$$
\vert p_{\theta'}(s_t) - p_{\theta}(s_t) \vert \le 2\epsilon t
$$</p>
<p>因而，improvement有上界</p>
<p>$$
J(\theta') - J(\theta) \le \sum_{t} 2\epsilon t C
$$</p>
<p>其中$r_{max}$是最大单步奖励，$C \in O(\frac{r_{max}}{1 -
\gamma})$。</p>
<p>最终，我们用$p_{\theta}$来替换
$p_{\theta'}$</p>
<p>$$
\begin{aligned}
\overline{A}(\theta') &= \sum_t E_{s_t \sim p_{\theta}(s_t)}\left[
E_{a_t \sim \pi_{\theta}(a_t \vert s_t)}\left[
\frac{\pi_{\theta'}(a_t \vert s_t)}{\pi_{\theta}(a_t \vert s_t)} \gamma^t
A^{\pi_{\theta}}(s_t, a_t) \right] \right]\\<br>
\theta' &\leftarrow \arg\max_{\theta'} \overline{A}(\theta')<br>
\end{aligned}
$$</p>
<h3 id=a-better-measure-of-divergence>A better measure of divergence</h3>
<p>在现实中，我们很难计算两个分布的total variation divergence，一般使用
<a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>KL Divergence</a>替代。</p>
<p>实际上，两个分布$\pi_{\theta'}(a_t \vert s_t)$和
$\pi_{\theta}(a_t \vert s_t)$的total variation divergence上界可以用KL divergence($D_{KL}$)的均方根来表示。</p>
<p>$$
\vert \pi_{\theta'}(a_t \vert s_t) - \pi_{\theta}(a_t \vert s_t) \vert \le
\sqrt{\frac{1}{2} D_{KL}\left(\pi_{\theta'}(a_t \vert s_t) \vert\vert
\pi_{\theta}(a_t \vert s_t)\right)}
$$</p>
<h2 id=enforcing-the-kl-constraint>Enforcing the KL constraint</h2>
<p>上文所有的证明都建立在$\pi'$和$\pi$之间的差异不大的基础上，因而需要有一个限制：</p>
<p>$$
D_{KL}\left(\pi_{\theta'(a_t \vert s_t)} \vert \vert \pi_{\theta}(a_t \vert s_t)\right)
\le \epsilon
$$</p>
<h3 id=dual-gradient-descent>Dual Gradient Descent</h3>
<p><a href=https://zhuanlan.zhihu.com/p/114574438>拉格朗日乘子法与对偶问题</a></p>
<p>构造原优化问题的对偶问题，一方面减少了约束，另一方面对偶问题是凸优化问题，一定有最优解。</p>
<p>原始的优化目标在引入拉格朗日乘子后可以转化为如下的形式：
$$
\label{eq:dual_opt}
\mathcal{L}(\theta', \lambda) = \overline{A}(\theta') - \lambda \left(D_{KL}\left(\pi_{\theta'}
(a_t \vert s_t) \vert\vert \pi_{\theta}(a_t \vert s_t)\right) - \epsilon\right)
$$</p>
<p>基于<strong>dual gradient descent</strong>算法：</p>
<ol>
<li>更新参数$\theta'$最大化$\mathcal{L}(\theta', \lambda)$（不必等到完全收敛，可以仅执行几步梯度更新）</li>
<li>$\lambda \leftarrow \lambda + \alpha \left( D_{KL}\left(\pi_{\theta'}(a_t \vert s_t)
\vert\vert \pi_{\theta}(a_t \vert s_t)\right) - \epsilon \right)$</li>
</ol>
<p>乘子$\lambda$也是一个需要优化的参数。当约束违反程度大的时候，后面拉格朗日项就变成特别大的负项，如果需要最大化这个目标，则需要将乘子变大，反之依然。通过对这个乘子的调节，进而修正约束部分的重要性，从而达到自适应优化的目标。</p>
<h3 id=natural-gradients>Natural Gradients</h3>
<p>除了将约束放进优化目标，另一个考虑的方向时修改目标函数。因为$\theta$和$\theta'$之间的差距很小，我们使用一阶泰勒展开在$\theta$处来近似$\theta'$。</p>
<p>即
$$
\begin{aligned}
\theta' &\leftarrow \arg\max_{\theta'} \overline{A}(\theta') \\<br>
\theta' &\leftarrow \arg\max_{\theta'}\nabla_\theta \overline{A}(\theta)^T(\theta'-\theta) <br>
\end{aligned}
$$</p>
<p>由
$$
\nabla_{\theta'}\overline{A}(\theta') =
\sum_t E_{s_t \sim p_{\theta}(s_t)}\left[ E_{a_t \sim p_{\theta}(a_t \vert s_t)} \left[
{\pi_{\theta'}(s_t,a_t) \over \pi_\theta(s_t,a_t)}
\gamma^t \nabla_{\theta'}\log\pi_{\theta'}(a_t \vert s_t) A^{\pi_{\theta}}(s_t, a_t)
\right]\right]
$$</p>
<p>得到
$$
\begin{aligned}
\nabla_{\theta}\overline{A}(\theta) &=
\sum_t E_{s_t \sim p_{\theta}(s_t)}\left[ E_{a_t \sim p_{\theta}(a_t \vert s_t)} \left[
{\pi_{\theta}(s_t,a_t) \over \pi_\theta(s_t,a_t)}
\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t \vert s_t) A^{\pi_{\theta}}(s_t, a_t)
\right]\right] \\<br>
&=
\sum_t E_{s_t \sim p_{\theta}(s_t)}\left[ E_{a_t \sim p_{\theta}(a_t \vert s_t)} \left[
\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t \vert s_t) A^{\pi_{\theta}}(s_t, a_t)
\right]\right] \\<br>
&= \nabla_\theta J(\theta)
\end{aligned}
$$</p>
<p>我们的优化目标：
$$
\begin{aligned}
\theta' \leftarrow \arg\max_{\theta'}\nabla_\theta J(\theta)^T(\theta'-\theta)\\<br>
s.t. D_{KL}\left(\pi_{\theta'(a_t \vert s_t)} \vert \vert \pi_{\theta}(a_t \vert s_t)\right)
\le \epsilon <br>
\end{aligned}
$$</p>
<p>对于gradient ascent来说，
$$
\begin{aligned}
\theta' \leftarrow \arg\max_{\theta'}\nabla_\theta J(\theta)^T(\theta'-\theta)\\<br>
s.t. \vert\vert \theta' - \theta \vert\vert^2 \le \epsilon <br>
\end{aligned}
$$</p>
<p>Taking a Policy Gradient step means that we are taking a step in a circular radius around
$\theta$, which is equivalent of maximizing $\overline{A}$ subject to
$$
\vert\vert \theta' - \theta \vert\vert^2 \le \epsilon
$$</p>
<p>natural gradients 使用二阶泰勒展开来近似KL Divergence：</p>
<p>$$
D_{KL}(\pi_{\theta'} \vert\vert \pi_{\theta}) \approx \frac{1}{2} (\theta' - \theta) \pmb{F}
(\theta' - \theta)
$$</p>
<p>其中$\pmb{F}$代表<a href=https://en.wikipedia.org/wiki/Fisher_information_metric#Relation_to_the_Kullback%E2%80%93Leibler_divergence>Fischer Information Matrix</a>，
让我们的优化区域变成了一个椭圆。</p>
<p><img src=/post/cs285_lecture9/gradients.png alt>
上图是使用欧式距离约束（左）和使用近似KL散度约束的对比图。</p>
<p>We transform our objective by $\pmb{F}$ inverse, such that the optimiation region becomes
again a circle and we can take a gradient step on this transformation. We call this the
<strong>Natural Gradient</strong>:</p>
<p>$$
\theta' = \theta + \alpha \pmb{F}^{-1} \nabla_{\theta}J(\theta)
$$
学习率$\alpha$：
$$
\alpha = \sqrt{\frac{2\epsilon}{\nabla_{\theta}J(\theta)^T\pmb{F}\nabla_{\theta}J(\theta)}}
$$</p>
<p>更多Natural Gradients参考
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.8735&rep=rep1&type=pdf">J. Peters, S. Schaal, Reinforcement learning of motor skills with policy gradients</a>。</p>
<p>Fischer Information
Matrix定义为：
$$
\pmb{F} = E_{\pi_{\theta}}\left[
\nabla_{\theta}\log\pi_{\theta}(\pmb{a}\vert\pmb{s})
\nabla_{\theta}\log\pi_{\theta}(\pmb{a}\vert\pmb{s})^T
\right]
$$</p>
<p>如果$\theta$具有一百万个参数，$\pmb{F}$就会是一百万乘上一百万的矩阵，对这个矩阵求逆显然不现实。</p>
<h3 id=trust-region-policy-optimization>Trust Region Policy Optimization</h3>
<p><a href=https://arxiv.org/pdf/1502.05477.pdf>Schulman et al., Trust Region Policy Optimization</a></p>
<p>TRPO主要针对NPG存在的两个问题提出了解决方案：第一个就是求逆矩阵的高复杂度问题，第二个则是对KL divergence做的approximation中可能存在的约束违反的问题做了预防。</p>
<p>首先就是求逆操作的高复杂度问题，TRPO将它转化为求解线性方程组，并利用conjugate gradient algorithm进行近似求解（这里有要求inverse matrix是positive definite的，所以在TRPO中有对目标函数的约束）。</p>
<p>而另外一点，如何对违反约束做限制，这里用的是exponential decay line search的方式，也就是对于一个样本，如果某次更新违反了约束，那么就把它的系数进行指数衰减，从而减少更新的幅度，直到它不再违反约束或者超过一定次数衰减则抛弃这次的数据。</p>
<h3 id=proximal-policy-optimization>Proximal Policy Optimization</h3>
<p><a href=https://arxiv.org/pdf/1707.06347.pdf>Schulman et al., Proximal Policy Optimization</a>,
提出了一种不要计算Fischer Information Matrix或者其近似值来满足$D_{KL}$限制的方法。</p>
<p>有两种方式：</p>
<ul>
<li>Clipping the surrogate objective</li>
<li>Adaptive KL Penalty Coefficient</li>
</ul>
<h4 id=clipping-the-surrogate-objective>Clipping the surrogate objective</h4>
<p>令$r(\theta)$为重要性采样权重，截断目标值为：
$$
L^{CLIP} = E_t \left[ \min\left(
r_r(\theta)A_t,
clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t
\right) \right]
$$</p>
<p>下图展示了$L^{CLIP}$在正的优势值和负优势值的情况。</p>
<p><img src=/post/cs285_lecture9/ppo_objective.png alt></p>
<p>但最近的文章如<a href="https://openreview.net/pdf?id=r1etN1rtPB">Engstrom et al., Implementation Matters in Deep Policy Gradients</a>
截断机制并没有阻止梯度步进破坏KL限制，提升点主要来自于
<strong>code-level optimizations</strong>，TRPO的表现其实更好。</p>
<h4 id=adaptive-kl-penalty-coefficient>Adaptive KL Penalty Coefficient</h4>
<p>将KL散度作为惩罚项加入目标函数。
策略更新主要包含以下两步：</p>
<ul>
<li>使用minibatch SGD更新KL-penalized目标。</li>
</ul>
<p>$
L^{KLPEN}(\theta) = E_t \left[ \frac{\pi_{\theta'}(a_t \vert s_t)}{\pi_{\theta}(a_t \vert s_t)}
A^{\pi_{\theta}}(s_t, a_t) - \beta D_{KL}(\pi_{\theta'}(. \vert s_t)\vert\vert
\pi_{\theta}(. \vert s_t))
\right]
$</p>
<ul>
<li>计算$d = E_t \left[ D_{KL}(\pi_{\theta'}(. \vert s_t)\vert\vert
\pi_{\theta}(. \vert s_t)) \right] $
<ul>
<li>If $d \lt d_{targ}/1.5$ then $\beta \leftarrow \beta/2$</li>
<li>If $d \gt 1.5 d_{targ}$ then $\beta \leftarrow 2\beta$</li>
</ul>
</li>
</ul>
<p>其中$d_{targ}$是期望KL Divergence目标值。</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/cs285_lecture10/lecture10/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">cs285 DRL notes lecture 10: Model-based Planning</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/cs285_lecture8/lecture8/>
<span class="next-text nav-default">cs285 DRL notes lecture 8: Deep RL with Q-Functions</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>