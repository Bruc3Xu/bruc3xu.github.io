<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>cs285 DRL notes chapter 2: imitation learning - RuChen Xu's Blog</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ruchen Xu"><meta name=description content="模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据"><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.88.1 with theme even">
<link rel=canonical href=http://localhost:1313/post/cs285_chapter2/cs285-drl-notes-chapter-2-imitation-learning/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="cs285 DRL notes chapter 2: imitation learning">
<meta property="og:description" content="模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/cs285_chapter2/cs285-drl-notes-chapter-2-imitation-learning/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2020-08-23T15:13:48+00:00">
<meta property="article:modified_time" content="2020-08-23T15:13:48+00:00">
<meta itemprop=name content="cs285 DRL notes chapter 2: imitation learning">
<meta itemprop=description content="模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据"><meta itemprop=datePublished content="2020-08-23T15:13:48+00:00">
<meta itemprop=dateModified content="2020-08-23T15:13:48+00:00">
<meta itemprop=wordCount content="1796">
<meta itemprop=keywords content="reinforcement learning,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="cs285 DRL notes chapter 2: imitation learning">
<meta name=twitter:description content="模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Blog</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Blog</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>cs285 DRL notes chapter 2: imitation learning</h1>
<div class=post-meta>
<span class=post-time> 2020-08-23 </span>
<div class=post-category>
<a href=/categories/cs285/> cs285 </a>
</div>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#daggerdataset-aggregation>DAgger(Dataset Aggregation)</a></li>
<li><a href=#模仿学习存在的为题和解决方法>模仿学习存在的为题和解决方法</a>
<ul>
<li><a href=#行为不具有马尔可夫性质>行为不具有马尔可夫性质</a></li>
<li><a href=#多模态行为>多模态行为</a></li>
</ul>
</li>
<li><a href=#误差分析>误差分析</a></li>
<li><a href=#总结>总结</a></li>
<li><a href=#参考>参考</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据专家策略采样得到的$o_t, a_t$，可以认为是输入和标签。</p>
<p>强化学习与监督学习的区别在于：</p>
<ol>
<li>强化学习中，数据并不是独立同分布的（i.i.d）。</li>
<li>强化学习并没有准确的标签，只有奖励值这一弱监督信号。</li>
</ol>
<p>模仿学习的一个问题是泛化能力差。例如，在状态$s_t$，智能体做出了一个错误决策（因为学习得到的策略分布与专家策略分布不能完全相同，这个问题是无法避免的），到达一个新的状态$s_t'$。这个状态对于智能体来说是没有见过的，即没有学习到的，那么智能体就会选择一个随机的动作，偏离学习到的轨迹。整个过程如下图所示。
<img src=/post/cs285_chapter2/distribution_shift.png alt></p>
<h2 id=daggerdataset-aggregation>DAgger(Dataset Aggregation)</h2>
<p>DAgger的思想是：既然没有见过的状态不在原有分布之内，那么我们使用专家策略对这个状态进行动作选择，并将其加入数据集进行训练，那么就可以解决上文提到的分布偏移问题。</p>
<hr>
<p>Human data D = {o_1,a_1,&mldr;,o_N,a_N}<br>
While not Converged<br>
$\quad$ Train $\pi_\theta(a_t|o_t)$ from human data $\mathcal{D} = {o_1,a_1,&mldr;,o_N,a_N}$<br>
$\quad$ Run $\pi_\theta(a_t|o_t)$ to get dataset $\mathcal{D}_\pi = {o_1,&mldr;,o_M}$<br>
$\quad$ Ask human to label $\mathcal{D}_\pi$ with actions $a_t$<br>
$\quad$ Aggregate $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D_\pi}$<br>
Return optimal imitation-learned trajectory as $\tau^{return}$</p>
<hr>
<p>DAgger算法存在的问题：</p>
<ol>
<li>无法保证人类标记数据的可靠性</li>
<li>人类做出决策不仅依赖当前状态，还依赖历史状态（并不一定指上一个状态），不满足马尔可夫性质。</li>
</ol>
<h2 id=模仿学习存在的为题和解决方法>模仿学习存在的为题和解决方法</h2>
<h3 id=行为不具有马尔可夫性质>行为不具有马尔可夫性质</h3>
<p>这个问题即DAgger算法的第二个问题。在这种情况下，策略学习由$\pi_{\theta}(a_t|o_t)$变为
$\pi_{\theta}(a_t|o_t, o_{t-1}, &mldr;)$。</p>
<p>一种解决方法是将历史状态堆叠起来作为新的状态。</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>LazyFrames</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>frames</span><span class=p>):</span>
        <span class=s2>&#34;&#34;&#34;This object ensures that common frames between the observations are only stored once.
</span><span class=s2>        It exists purely to optimize memory usage which can be huge for DQN&#39;s 1M frames replay
</span><span class=s2>        buffers.
</span><span class=s2>        This object should only be converted to numpy array before being passed to the model.
</span><span class=s2>        You&#39;d not believe how complex the previous solution was.&#34;&#34;&#34;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_frames</span> <span class=o>=</span> <span class=n>frames</span>

    <span class=k>def</span> <span class=nf>__array__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_frames</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
        <span class=k>if</span> <span class=n>dtype</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>out</span> <span class=o>=</span> <span class=n>out</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>dtype</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

<span class=k>class</span> <span class=nc>FrameStack</span><span class=p>(</span><span class=n>gym</span><span class=o>.</span><span class=n>Wrapper</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>env</span><span class=p>,</span> <span class=n>k</span><span class=p>):</span>
        <span class=s2>&#34;&#34;&#34;Stack k last frames.
</span><span class=s2>        Returns lazy array, which is much more memory efficient.
</span><span class=s2>        See Also
</span><span class=s2>        --------
</span><span class=s2>        baselines.common.atari_wrappers.LazyFrames
</span><span class=s2>        &#34;&#34;&#34;</span>
        <span class=n>gym</span><span class=o>.</span><span class=n>Wrapper</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>env</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>k</span> <span class=o>=</span> <span class=n>k</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>frames</span> <span class=o>=</span> <span class=n>deque</span><span class=p>([],</span> <span class=n>maxlen</span><span class=o>=</span><span class=n>k</span><span class=p>)</span>
        <span class=n>shp</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>observation_space</span><span class=o>.</span><span class=n>shape</span>
        <span class=c1># typically image observation</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>observation_space</span> <span class=o>=</span> <span class=n>spaces</span><span class=o>.</span><span class=n>Box</span><span class=p>(</span><span class=n>low</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>high</span><span class=o>=</span><span class=mi>255</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>shp</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>shp</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>shp</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>*</span> <span class=n>k</span><span class=p>))</span>

    <span class=k>def</span> <span class=nf>_reset</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>ob</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k</span><span class=p>):</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>ob</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_ob</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
        <span class=n>ob</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>ob</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_ob</span><span class=p>(),</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span><span class=p>,</span> <span class=n>info</span>

    <span class=k>def</span> <span class=nf>_get_ob</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=p>)</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>k</span>
        <span class=k>return</span> <span class=n>LazyFrames</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=p>))</span>
</code></pre></td></tr></table>
</div>
</div><p>另一种方法是使用循环神经网络来处理时序输入，一般使用LSTM网络。</p>
<p>但更多的历史状态作为输入很容易导致<strong>因果混淆</strong>。例如，当一辆自动驾驶车辆遇到障碍物，踩下刹车时，刹车灯会亮起。智能体更容易通过刹车灯的亮灭而不是障碍物来进行决策。DAgger因为使用人为标记来扩充数据集，不会出现此类问题。</p>
<h3 id=多模态行为>多模态行为</h3>
<p>专家策略可能会出现多模态行为，例如，针对车辆前方的障碍物，可能会选择向左变道或者向右变道。这时，如果学习的策略是一个概率分布，那么在这个状态下的动作就会被平均，趋向于不变道，这显然是错误的。
解决方法：</p>
<ol>
<li>使用多个分布的累加代替单一策略分布，一般使用混合高斯分布。</li>
<li>将状态映射到latent空间。</li>
<li>使用自回归离散化 (Autoregressive Discretization)。</li>
</ol>
<h2 id=误差分析>误差分析</h2>
<p>假设我们可以寻找到一个策略$\pi$ ，其与最优策略的损失函数值小于给定的精度$\epsilon$，那么我们可以证明出来，这个策略与专家策略的决策质量上有如下的保证：
$$
V(\pi_E)-V(\pi)&lt;=\frac{2\sqrt{2}}{(1-\gamma)^2}\sqrt{\epsilon}
$$
可以看到，损失函数值越小，两者的值函数差异越小。但与此同时，我们注意到这个差异是以$1/(1-\gamma)^2$的速度在放大。这个现象在模仿学习中被称作为”复合误差“ (compounding errors）：对于一个有效决策长度（以$1-\gamma$来衡量， $\gamma$越接近1，有效决策长度越长）的模仿学习任务，值函数值差异随目标函数值差异以二次方的速度增长。也就说：对于有效决策长度比较长的任务来讲，即使我们把目标函数优化地很小，值函数的差异依然可能很大。这个结论在以前的paper<a href=#refer-anchor><sup>1, 2</sup></a>和最近的paper<a href=#refer-anchor><sup>3</sup></a>里都有详细的描述。</p>
<p>由于数据增广和环境交互，DAgger 算法会大大减小未访问的状态的个数，从而减小复合误差。</p>
<h2 id=总结>总结</h2>
<p>总结来说，模仿学习通常有一定局限性（分布不匹配的问题、误差累积问题），但有时候能做得不错，如使用一些稳定控制器，或者从稳定轨迹分布中抽样，亦或是使用DAgger之类的算法增加更多的在线数据，理想化地如使用更好的模型来拟合得更完美。</p>
<p>更进一步的关于模仿学习的内容，可以参考<a href=https://www.lamda.nju.edu.cn/xut/Imitation_Learning.pdf>Imitation Learning</a>这本书。</p>
<h2 id=参考>参考</h2>
<div id=refer-anchor></div>
[1]Ross, Stéphane, Geoffrey Gordon, and Drew Bagnell. "A reduction of imitation learning and structured prediction to no-regret online learning." Proceedings of the 14th international conference on artificial intelligence and statistics. 2011.
<p>[2]Syed, Umar, and Robert E. Schapire. &ldquo;A reduction from apprenticeship learning to classification.&ldquo;Advances in neural information processing systems. 2010.</p>
<p>[3]Xu, Tian, Ziniu Li, and Yang Yu. &ldquo;Error Bounds of Imitating Policies and Environments.&rdquo; Advances in neural information processing systems. 2020.</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ruchen Xu</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2020-08-23
</span>
</p>
</div>
<div class=post-reward>
<input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>Reward</label>
<div class=qr-code>
<label class=qr-code-image for=reward>
<img class=image src=/img/reward/wechat.png>
<span>wechat</span>
</label>
<label class=qr-code-image for=reward>
<img class=image src=/img/reward/alipay.png>
<span>alipay</span>
</label>
</div>
</div><footer class=post-footer>
<div class=post-tags>
<a href=/tags/reinforcement-learning/>reinforcement learning</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/pybind11_cmake_tutorial/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">Pybind11 Cmake tutorial</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/cs285_chapter1/cs285-drl-notes-chapter-1-introduction/>
<span class="next-text nav-default">cs285 DRL notes chapter 1: introduction</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ruchenxucs@gmail.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/FireLandDS class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ruchen Xu</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
</body>
</html>