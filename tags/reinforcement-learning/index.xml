<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>reinforcement learning on RuChen Xu's Blog</title><link>http://localhost:1313/tags/reinforcement-learning/</link><description>Recent content in reinforcement learning on RuChen Xu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 13 Nov 2020 09:34:56 +0800</lastBuildDate><atom:link href="http://localhost:1313/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Soft Actor Critic</title><link>http://localhost:1313/post/soft_actor_critic/soft-actor-critic/</link><pubDate>Fri, 13 Nov 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/soft_actor_critic/soft-actor-critic/</guid><description>&lt;p>现有深度强化学习算法主要的问题有：&lt;/p>
&lt;ul>
&lt;li>采样难，样本利用率低：对于一般的强化学习问题，学习得到想要的策略需要的样本以百万、千万记，而绝大多数on-policy算法在策略更新后丢弃旧的样本。&lt;/li>
&lt;/ul></description></item><item><title>Distributed RL</title><link>http://localhost:1313/post/distributed_rl/distributed_rl/</link><pubDate>Wed, 21 Oct 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/distributed_rl/distributed_rl/</guid><description>&lt;p>分布式强化学习算法可以大幅提升采样效率，加速学习速度，对于on-policy算法一定程度也能减少方差。&lt;/p></description></item><item><title>cs285 DRL notes lecture 10: Model-based Planning</title><link>http://localhost:1313/post/cs285_lecture10/lecture10/</link><pubDate>Tue, 29 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture10/lecture10/</guid><description>&lt;p>model-free强化学习忽略了状态转移概率$p(s_{t+1}|s_t,a_t)$，因为实际情况环境的模型往往无法获得或者学习。但也有一些例外：&lt;/p></description></item><item><title>cs285 DRL notes lecture 9: Advanced Policy Gradients</title><link>http://localhost:1313/post/cs285_lecture9/lecture9/</link><pubDate>Wed, 23 Sep 2020 09:34:56 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture9/lecture9/</guid><description>&lt;p>本章会深入策略梯度算法，进一步学习
&lt;strong>Natural Policy Gradient&lt;/strong>, &lt;strong>Trust Region Policy Optimization&lt;/strong>, &lt;strong>Proximal Policy
Optimization&lt;/strong>等算法。&lt;/p></description></item><item><title>cs285 DRL notes lecture 8: Deep RL with Q-Functions</title><link>http://localhost:1313/post/cs285_lecture8/lecture8/</link><pubDate>Fri, 18 Sep 2020 15:46:38 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture8/lecture8/</guid><description>&lt;p>fitted q iteration与Q-Learning不同&lt;/p>
&lt;ul>
&lt;li>fitted q iteration算法：当前策略收集整个数据集，然后对Q函数进行多次回归近似，接下来收集新的数据集循环这一过程。&lt;/li>
&lt;li>Q-Learning（online q iteration）：一边收集数据，一边进行学习。&lt;/li>
&lt;/ul></description></item><item><title>cs285 DRL notes lecture 7: value function methods</title><link>http://localhost:1313/post/cs285_lecture7/lecture7/</link><pubDate>Wed, 16 Sep 2020 09:43:42 +0800</pubDate><guid>http://localhost:1313/post/cs285_lecture7/lecture7/</guid><description>&lt;p>本章学习基于值函数的强化学习方法。&lt;/p></description></item><item><title>cs285 DRL notes lecture 6: Actor-Critic methods</title><link>http://localhost:1313/post/cs285_lecture6/actor-critic-methods/</link><pubDate>Tue, 08 Sep 2020 17:18:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture6/actor-critic-methods/</guid><description>回顾策略梯度算法， $$\nabla_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T\nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\right)\left(\sum_{t'=t}^T r(s_{i,t'},a_{i,t'})\right)$$ 我们使用'&amp;lsquo;reward-to-go&amp;rsquo;' 来近似在状态$s_{i,t}$采取动作 $a_{</description></item><item><title>cs285 DRL notes chapter 3: policy gradient methods</title><link>http://localhost:1313/post/cs285_lecture5/pg/</link><pubDate>Sat, 29 Aug 2020 16:00:06 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture5/pg/</guid><description>回顾强化学习的目标，我们希望获得策略的最优参数$\theta^*$， $$ \theta^*=\underset{\theta}{argmax}\mathbb{E}_{\tau\sim p_{\theta}(\tau)}[\sum_{t=1}^{t=T}r(s_t, a_t)] $$ 这实际上是一个优化问题，因此我们可以使用多种优化方法来优化这个</description></item><item><title>cs285 DRL notes lecture 2: imitation learning</title><link>http://localhost:1313/post/cs285_lecture2/lecture2/</link><pubDate>Sun, 23 Aug 2020 15:13:48 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture2/lecture2/</guid><description>模仿学习是一种监督学习方法，行为克隆是其中的一类方法。其基本思想是从专家演示数据中学习到一个尽可能接近专家策略的行为策略。我们的数据集是依据</description></item><item><title>cs285 DRL notes lecture 4: RL introduction</title><link>http://localhost:1313/post/cs285_lecture4/lecture3/</link><pubDate>Mon, 03 Aug 2020 10:58:41 +0000</pubDate><guid>http://localhost:1313/post/cs285_lecture4/lecture3/</guid><description>强化学习是一种目标导向的学习方法，通过不断试错，奖励或惩罚智能体从而使其未来更容易重复或者放弃某一动作。 强化学习中的术语介绍。 强化学习的主要</description></item></channel></rss>